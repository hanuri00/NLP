{"cells":[{"cell_type":"markdown","metadata":{"id":"wDR91oy247r1"},"source":["# 9주 2차시. RNN을 이용한 문서 분류 - IMDB 리뷰 감성분석\n"]},{"cell_type":"code","source":["!apt install python3.7"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBOpSNcVe1hM","executionInfo":{"status":"ok","timestamp":1694154632911,"user_tz":-540,"elapsed":14860,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"}},"outputId":"1128b54f-047d-47a5-8c22-77ed137e3b6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  libpython3.7-minimal libpython3.7-stdlib mailcap mime-support\n","  python3.7-minimal\n","Suggested packages:\n","  python3.7-venv binfmt-support\n","The following NEW packages will be installed:\n","  libpython3.7-minimal libpython3.7-stdlib mailcap mime-support python3.7\n","  python3.7-minimal\n","0 upgraded, 6 newly installed, 0 to remove and 16 not upgraded.\n","Need to get 4,698 kB of archives.\n","After this operation, 17.8 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n","Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-minimal amd64 3.7.17-1+jammy1 [608 kB]\n","Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-minimal amd64 3.7.17-1+jammy1 [1,837 kB]\n","Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-stdlib amd64 3.7.17-1+jammy1 [1,864 kB]\n","Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7 amd64 3.7.17-1+jammy1 [362 kB]\n","Fetched 4,698 kB in 4s (1,235 kB/s)\n","Selecting previously unselected package libpython3.7-minimal:amd64.\n","(Reading database ... 120901 files and directories currently installed.)\n","Preparing to unpack .../0-libpython3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n","Unpacking libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n","Selecting previously unselected package python3.7-minimal.\n","Preparing to unpack .../1-python3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n","Unpacking python3.7-minimal (3.7.17-1+jammy1) ...\n","Selecting previously unselected package mailcap.\n","Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n","Unpacking mailcap (3.70+nmu1ubuntu1) ...\n","Selecting previously unselected package mime-support.\n","Preparing to unpack .../3-mime-support_3.66_all.deb ...\n","Unpacking mime-support (3.66) ...\n","Selecting previously unselected package libpython3.7-stdlib:amd64.\n","Preparing to unpack .../4-libpython3.7-stdlib_3.7.17-1+jammy1_amd64.deb ...\n","Unpacking libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n","Selecting previously unselected package python3.7.\n","Preparing to unpack .../5-python3.7_3.7.17-1+jammy1_amd64.deb ...\n","Unpacking python3.7 (3.7.17-1+jammy1) ...\n","Setting up libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n","Setting up python3.7-minimal (3.7.17-1+jammy1) ...\n","Setting up mailcap (3.70+nmu1ubuntu1) ...\n","Setting up mime-support (3.66) ...\n","Setting up libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n","Setting up python3.7 (3.7.17-1+jammy1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n"]}]},{"cell_type":"code","source":["#!pip install torchtext\n","!pip install -U torchtext==0.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKYV21NdsQIa","executionInfo":{"status":"ok","timestamp":1694154688342,"user_tz":-540,"elapsed":7821,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"}},"outputId":"3b1f126a-aaad-4278-b159-1cff3a0ec3d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchtext==0.6.0\n","  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m61.4/64.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n","Collecting sentencepiece (from torchtext==0.6.0)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n","Installing collected packages: sentencepiece, torchtext\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.15.2\n","    Uninstalling torchtext-0.15.2:\n","      Successfully uninstalled torchtext-0.15.2\n","Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"]}]},{"cell_type":"code","source":["%%capture\n","!python -m spacy download en"],"metadata":{"id":"ys51sncufRWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torchtext import data\n","\n","TEXT = data.Field(tokenize = 'spacy',\n","                  tokenizer_language = 'en')\n","LABEL = data.LabelField(dtype = torch.float) # pos -> 1 / neg -> 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":515},"id":"TlEFZsExfS-H","executionInfo":{"status":"error","timestamp":1694154735168,"user_tz":-540,"elapsed":7343,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"}},"outputId":"d845760c-504f-48d5-a796-4a9e6a041eba"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-f7ef48453080>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m TEXT = data.Field(tokenize = 'spacy',\n\u001b[0m\u001b[1;32m      5\u001b[0m                   tokenizer_language = 'en')\n\u001b[1;32m      6\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# pos -> 1 / neg -> 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sequential, use_vocab, init_token, eos_token, fix_length, dtype, preprocessing, postprocessing, lower, tokenize, tokenizer_language, include_lengths, batch_first, pad_token, unk_token, pad_first, truncate_first, stop_words, is_target)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# in case the tokenizer isn't picklable (e.g. spacy)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minclude_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchtext/data/utils.py\u001b[0m in \u001b[0;36mget_tokenizer\u001b[0;34m(tokenizer, language)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mspacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_spacy_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'en'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models and if you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"en\")"]}]},{"cell_type":"code","source":["#IMDB Datasets\n","#5만개 영화 리뷰"],"metadata":{"id":"ELhYea0wfTBS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchtext import datasets\n","\n","train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"],"metadata":{"id":"cgCpHMvjfTEc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'training examples 수 : {len(train_data)}') #25,000\n","print(f'testing examples 수 : {len(test_data)}') #25,000\n","\n","print(vars(train_data.examples[0]))"],"metadata":{"id":"gHvYgLp_fTHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","\n","SEED = 1234\n","\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","train_data, valid_data = train_data.split(random_state = random.seed(SEED))"],"metadata":{"id":"7XBWL7UXfTKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'training examples 수 : {len(train_data)}')\n","print(f'validations examples 수 : {len(valid_data)}')\n","print(f'testing examples 수 : {len(test_data)}')"],"metadata":{"id":"b4XCfsY_fxEz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Build Vocabulary\n","MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, min_freq = 5)\n","LABEL.build_vocab(train_data)"],"metadata":{"id":"txpfRU9kfxIJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n","print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"],"metadata":{"id":"QpRx-Ko8fxLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"가장 자주 나오는 단어들 20개 출력 :\\n{TEXT.vocab.freqs.most_common(20)}\\n\")\n","\n","# itos(int to string)\n","print(TEXT.vocab.itos[:5])\n","\n","# stoi(string to int)\n","print(LABEL.vocab.stoi)"],"metadata":{"id":"WKRI2hydfxOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(torch.__version__)"],"metadata":{"id":"Ex8wA5OhfxRN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"wHNKwL_kfxUX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 64\n","\n","train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n","    (train_data, valid_data, test_data),\n","    batch_size = BATCH_SIZE,\n","    device = device\n",")"],"metadata":{"id":"mriY_u_efTNe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# iterator 출력\n","for i, batch in enumerate(train_iterator):\n","    text = batch.text\n","    label = batch.label\n","\n","    print(f\"첫 번째 배치의 text 크기: {text.shape}\")\n","    print(text[3])\n","    print(text[3].shape)\n","    print(f\"첫 번째 배치의 label 크기: {label.shape}\")\n","    print(label)\n","\n","    # 첫 번째 batch만 출력\n","    break"],"metadata":{"id":"kTHp8sl0griZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Bulid Model\n","import torch.nn as nn\n","\n","class RNN(nn.Module):\n","  def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n","    super().__init__()\n","\n","    self.embedding = nn.Embedding(input_dim, embedding_dim)\n","    self.rnn = nn.RNN(embedding_dim, hidden_dim)\n","    self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","  def forward(self, text):\n","\n","    # text = [sentence length, batch size]\n","\n","    embedded = self.embedding(text)\n","\n","    # embedded = [sentence length, batch size, embedding dim]\n","\n","    output, hidden = self.rnn(embedded)\n","\n","    # output = [sentence length, batch size, hidden dim]\n","    # hidden = [1, batch size, hidden dim]\n","\n","    return self.fc(hidden.squeeze(0))"],"metadata":{"id":"ZCD63TVggrlS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = len(TEXT.vocab) #25,002\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 1\n","\n","model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"],"metadata":{"id":"qxhbIcIlgrnw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"metadata":{"id":"E8Wxfv8Cgrq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the Model\n","import torch.optim as optim\n","\n","optimizer =optim.SGD(model.parameters(), lr = 1e-3)\n","\n","criterion = nn.BCEWithLogitsLoss()\n","\n","# GPU\n","model = model.to(device)\n","criterion = criterion.to(device)"],"metadata":{"id":"cQeRtOQRg3we"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def binary_accuracy(preds, y):\n","\n","  rounded_preds = torch.round(torch.sigmoid(preds))\n","  # rounded_preds : [batch size]\n","  # y : batch.label\n","  correct = (rounded_preds == y).float()\n","  acc = correct.sum() / len(correct)\n","  return acc"],"metadata":{"id":"RciWQLBXg3zY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train\n","def train(model, iterator, optimizer, criterion):\n","\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  # model을 \"training mode\"로 -> dropout이나 batch normalization이 가능해짐\n","  # 이 모델에서는 이를 사용하지는 않음\n","  model.train()\n","\n","  for batch in iterator:\n","\n","    # 모든 batch마다 gradient를 0으로 초기화\n","    optimizer.zero_grad()\n","\n","    # batch of sentences인 batch.text를 model에 입력 (저절로 forward가 됨)\n","    # predictions의 크기가 [batch size, 1]이므로 squeeze해서 [batch size]로 size를 변경해줘야 함\n","    predictions = model(batch.text).squeeze(1)\n","\n","    # prediction결과와 batch.label을 비교하여 loss값 계산\n","    loss = criterion(predictions, batch.label)\n","\n","    # 정확도 계산\n","    acc = binary_accuracy(predictions, batch.label)\n","\n","    # backward()를 사용하여 역전파 수행\n","    loss.backward()\n","\n","    # 최적화 알고리즘을 사용하여 parameter를 update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"metadata":{"id":"Hf2ZBmW4g32N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, iterator, criterion):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  # \"evaluation mode\" : dropout이나 batch nomalizaation을 끔\n","  model.eval()\n","\n","  # pytorch에서 gradient가 계산되지 않도록 해서 memory를 적게 쓰고 computation 속도를 높임\n","  with torch.no_grad():\n","\n","    for batch in iterator :\n","      predictions = model(batch.text).squeeze(1)\n","\n","      loss = criterion(predictions, batch.label)\n","      acc = binary_accuracy(predictions, batch.label)\n","\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"metadata":{"id":"Ix_hMbEhg34y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"metadata":{"id":"xMJ9qhYKhJkO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N_EPOCHS = 5\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n","    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'tut1-model.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"],"metadata":{"id":"bIm8PmDrgrtn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('tut1-model.pt'))\n","\n","test_loss, test_acc = evaluate(model, test_iterator, criterion)\n","\n","print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"],"metadata":{"id":"vXQYweUshQlx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------------------------------------------------------"],"metadata":{"id":"5zI9xhzPgrw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchtext import data, datasets, legacy\n","import random"],"metadata":{"id":"kE9dy8Vt5WvW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6f350Hl47r_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694142072519,"user_tz":-540,"elapsed":4,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"}},"outputId":"e278de58-d56b-4911-c2ef-6772631fabeb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7c5abcfcf870>"]},"metadata":{},"execution_count":30}],"source":["SEED = 5\n","random.seed(SEED)\n","torch.manual_seed(SEED)"]},{"cell_type":"code","source":["# 하이퍼파라미터\n","BATCH_SIZE = 64\n","lr = 0.001\n","EPOCHS = 10"],"metadata":{"id":"6lVtK1TfrBAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["USE_CUDA = torch.cuda.is_available()\n","DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","print(\"cpu와 cuda 중 다음 기기로 학습함:\", DEVICE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OiPxOdaUrCgY","executionInfo":{"status":"ok","timestamp":1694142075831,"user_tz":-540,"elapsed":1,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"}},"outputId":"1fd2e5c6-6392-48ef-c118-f0d4b867c781"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu와 cuda 중 다음 기기로 학습함: cpu\n"]}]},{"cell_type":"code","source":["TEXT = legacy.data.Field(sequential=True, batch_first=True, lower=True)\n","LABEL = legacy.data.Field(sequential=False, batch_first=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"8dwx-PqdrE3E","executionInfo":{"status":"error","timestamp":1694142109196,"user_tz":-540,"elapsed":6,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"}},"outputId":"0b264fd3-1c41-4573-a117-275667a4b584"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-6c5a66181c36>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#TEXT = data.Field(sequential=True, batch_first=True, lower=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mTEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Field' is not defined"]}]},{"cell_type":"code","source":["# 전체 데이터를 훈련 데이터와 테스트 데이터를 8:2 비율로 나누기\n","trainset, testset = legacy.datasets.IMDB.splits(TEXT, LABEL)\n","# 이를 바탕으로 trainset, test set에는 리뷰와 label의 쌍이 들어가게 된다."],"metadata":{"id":"LDqsem-Yv1Es"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 단어 집합의 생성\n","# min_freq를 통해서 5번도 안나온 단어는 <unk>가 될 것이다.\n","TEXT.build_vocab(trainset, min_freq=5) # 단어 집합 생성\n","LABEL.build_vocab(trainset)\n","\n","trainset, valset = trainset.split(split_ratio=0.8)"],"metadata":{"id":"fnYu5MFav1G_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_iter, val_iter, test_iter = legacy.data.BucketIterator.splits(\n","        (trainset, valset, testset), batch_size=BATCH_SIZE,\n","        shuffle=True, repeat=False)"],"metadata":{"id":"gv1WOddpv1J0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#신경망 구성\n"],"metadata":{"id":"PGa_wla3v1Mo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lass GRU(nn.Module):\n","    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n","        super(GRU, self).__init__()\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","\n","        self.embed = nn.Embedding(n_vocab, embed_dim)\n","        self.dropout = nn.Dropout(dropout_p)\n","        # 앞에서 batch_first를 true로 했으므로, 여기서도 batch_first = True로 통일.\n","        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n","                          num_layers=self.n_layers,\n","                          batch_first=True)\n","        # 최종 은닉층에서, 답을 내기 위한 과정이다.\n","        self.out = nn.Linear(self.hidden_dim, n_classes)\n","\n","    def forward(self, x):\n","        x = self.embed(x)\n","        # 중요! 최초의 hidden_state를 정의하는 과정을 넣어줘야 한다.\n","        # 첫번째 히든 스테이트를 0벡터로 초기화\n","        h_0 = self._init_state(batch_size=x.size(0))\n","\n","        # GRU의 리턴값은 (배치 크기, 시퀀스 길이, 은닉 상태의 크기)\n","        x, _ = self.gru(x, h_0)\n","\n","        # (배치 크기, 은닉 상태의 크기)의 텐서로 크기가 변경됨. 즉, 마지막 time-step의 은닉 상태만 가져온다.\n","        h_t = x[:,-1,:]\n","        self.dropout(h_t)\n","\n","        # (배치 크기, 은닉 상태의 크기) -> (배치 크기, 출력층의 크기)\n","        logit = self.out(h_t)\n","        return logit\n","\n","    def _init_state(self, batch_size=1):\n","        weight = next(self.parameters()).data\n","        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()"],"metadata":{"id":"ntDytNCGv1PN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(TEXT.vocab)\n","n_classes = 2\n","\n","model = GRU(1, 256, vocab_size, 128, n_classes, 0.5)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"],"metadata":{"id":"8dpDgtTFv1R5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, optimizer, train_iter):\n","    model.train()\n","    for b, batch in enumerate(train_iter):\n","        # 배치에서 text와 label을 가져온다.\n","        x, y = batch.text, batch.label\n","        y.data.sub_(1)  # 레이블 값을 0과 1로 변환\n","        optimizer.zero_grad()\n","\n","        # 학습 과정을 통해 0, 1 결정,  binary이므로cross_entropy\n","        logit = model(x)\n","        loss = F.cross_entropy(logit, y)\n","\n","        # back-propagation\n","        loss.backward()\n","        optimizer.step()\n","def evaluate(model, val_iter):\n","    \"\"\"evaluate model\"\"\"\n","    model.eval()\n","    corrects, total_loss = 0, 0\n","    for batch in val_iter:\n","        x, y = batch.text, batch.label\n","        y.data.sub_(1) # 레이블 값을 0과 1로 변환\n","        logit = model(x)\n","        loss = F.cross_entropy(logit, y, reduction='sum')\n","\n","        # error를 더하고 data개수로 나눠서, 평균 error를 평가한다.\n","        total_loss += loss.item()\n","        corrects += (logit.max(1)[1].view(y.size()).data == y.data).sum()\n","    size = len(val_iter.dataset)\n","    avg_loss = total_loss / size\n","    avg_accuracy = 100.0 * corrects / size\n","    return avg_loss, avg_accuracy"],"metadata":{"id":"p2FXhpYIwA8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_val_loss = None\n","for e in range(1, EPOCHS+1):\n","    train(model, optimizer, train_iter)\n","    val_loss, val_accuracy = evaluate(model, val_iter)\n","\n","    print(\"[Epoch: %d] val loss : %5.2f | val accuracy : %5.2f\" % (e, val_loss, val_accuracy))\n","\n","    # 검증 오차가 가장 적은 최적의 모델을 저장\n","    if not best_val_loss or val_loss < best_val_loss:\n","        if not os.path.isdir(\"snapshot\"):\n","            os.makedirs(\"snapshot\")\n","        torch.save(model.state_dict(), './snapshot/txtclassification.pt')\n","        best_val_loss = val_loss"],"metadata":{"id":"Vi3ddOoewA-n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LZighZnMwBBd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XaM_3wKvwBEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rNcBlnsOwBHD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BXhI0OBNwBKC"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[{"file_id":"1yrXv8T0JcrVK-UbtEDGub9oREE98GWR5","timestamp":1694077552756}]}},"nbformat":4,"nbformat_minor":0}