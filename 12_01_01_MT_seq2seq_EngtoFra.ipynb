{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyN565Ca37CQ09Qk2yWwM7zG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine Translation"],"metadata":{"id":"vurN7dzFq4h_"}},{"cell_type":"markdown","source":["## 라이브러리 호출"],"metadata":{"id":"zS3CJOMcuxTM"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pandas as pd\n","\n","import os\n","\n","import re\n","import random\n","\n","device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"],"metadata":{"id":"pN8wy_E5q2uz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## dataset"],"metadata":{"id":"UtESYoMivW80"}},{"cell_type":"code","source":["# 타토에바 프로젝트 중 영어-프랑스어\n","# http://www.nanythings.org/anki\n","# tab으로 구분"],"metadata":{"id":"TjEfFbSevk1k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터 준비"],"metadata":{"id":"VsKHKftzxP6A"}},{"cell_type":"code","source":["SOS_token = 0\n","EOS_token = 1\n","MAX_LENGTH = 20\n","\n","class Lang:\n","    def __init__(self):\n","        self.word2index = {}\n","        self.word2count = {}\n","        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n","        # SOS - 문장의 시작, EOS - 문장의 끝\n","        self.n_words = 2 # sos, eos 카운트\n","\n","    def addSentence(self, sentence):        # 문장을 단어 단위로 분리, 컨테이너에 추가\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):                            #컨테이너에 단어가 없으면 추가, 있을 시 카운터 +1\n","        if word not in self.word2index:\n","            self.word2index[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.index2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1"],"metadata":{"id":"cwYcFoyjrPWg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 데이터 정규화"],"metadata":{"id":"0t0KNy46xYyI"}},{"cell_type":"code","source":["# 판다스를 활용 정규\n","def normalizeString(df, lang) :\n","    sentence = df[lang].str.lower() # 소문자 전환\n","    sentence = sentence.str.replace('[^A-Za-z\\s]+', ' ')\n","    sentence = sentence.str.normalize('NFD') # 유니코드 정규화\n","    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n","    return sentence\n","\n","def read_sentence(df, lang1, lang2) :\n","    sentence1 = normalizeString(df, lang1) # 데이터셋 1번째 열\n","    sentence2 = normalizeString(df, lang2)\n","    return sentence1, sentence2\n","\n","def read_file(loc, lang1, lang2) :\n","    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n","    return df\n","\n","def process_data(lang1, lang2) :\n","    df = read_file('../080289-main/chap10/data/%s-%s.txt'%(lang1, lang2), lang1, lang2) # load data\n","    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n","\n","    input_lang = Lang()\n","    output_lang = Lang()\n","    pairs = []\n","    for i in range(len(df)):\n","        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n","            full = [sentence1[i], sentence2[i]] # 1, 2열 합쳐서 저장\n","            input_lang.addSentence(sentence1[i]) # input으로 영어 사용\n","            output_lang.addSentence(sentence2[i]) # output으로 프랑스어 사용\n","            pairs.append(full) # 입, 출력 합쳐서 사용\n","\n","    return input_lang, output_lang, pairs"],"metadata":{"id":"MX-Fma7trPev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## tensor로 변환"],"metadata":{"id":"fnxBgP93xyKJ"}},{"cell_type":"code","source":["# 영어-프랑스어의 데이터쌍을 Tensor로 변환\n","\n","def indexesFromSentence(lang, sentence): # 문장 분리 및 인덱스 반환\n","    return [lang.word2index[word] for word in sentence.split(' ')]\n","\n","def tensorFromSentence(lang, sentence): # 문장 끝에 토큰 추가\n","    indexes = indexesFromSentence(lang, sentence)\n","    indexes.append(EOS_token)\n","    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n","\n","def tensorsFromPair(input_lang, output_lang, pair): # 입출력문장 텐서로 변환\n","    input_tensor = tensorFromSentence(input_lang, pair[0])\n","    target_tensor = tensorFromSentence(output_lang, pair[1])\n","    return (input_tensor, target_tensor)"],"metadata":{"id":"FHOKL6ffrPiG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## encoder"],"metadata":{"id":"72l3kwue1rws"}},{"cell_type":"code","source":["# 인코더 네트워크\n","# encoder = embedding layer + GRU layer\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n","        super(Encoder, self).__init__()\n","        self.input_dim = input_dim # 인코더 입력층\n","        self.embbed_dim = embbed_dim # 인코더 임베딩 계층\n","        self.hidden_dim = hidden_dim # 인코더 은닉층(이전 은닉층)\n","        self.num_layers = num_layers # GRU 계층 개수\n","        self.embedding = nn.Embedding(input_dim, self.embbed_dim) # 임베딩 계층 초기화\n","        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n","        # 임베딩 차원, 은닉층 차원, gru 계층 개수를 이용하여 gru 계층 초기화\n","\n","    def forward(self, src):\n","        embedded = self.embedding(src).view(1,1,-1) # 임베딩\n","        outputs, hidden = self.gru(embedded) # 임베딩 결과를 GRU 모델에 적용\n","        return outputs, hidden"],"metadata":{"id":"aFNGJlDIrPlZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## decoder\n","decoder = embedding layer + GRU layer + linear layer\n","\n","디코더 네트워크를 정의한다. logSoftmax는 소프트맥스와 log 함수의 결합으로, 소프트맥스 활성화 함수에서 발생할 수 있는 기울기 소멸 문제를 방지하기 위해 만들어진 활성화 함수이다."],"metadata":{"id":"eHKBpWt02P_r"}},{"cell_type":"code","source":["# 디코더 네트워크\n","\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n","        super(Decoder, self).__init__()\n","\n","        self.embbed_dim = embbed_dim\n","        self.hidden_dim = hidden_dim\n","        self.output_dim = output_dim\n","        self.num_layers = num_layers\n","\n","        self.embedding = nn.Embedding(output_dim, self.embbed_dim) # 임베딩 초기화\n","        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers) # gru 초기화\n","        self.out = nn.Linear(self.hidden_dim, output_dim) # 선형 계층 초기화\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, hidden):\n","        input = input.view(1, -1)\n","        embedded = F.relu(self.embedding(input))\n","        output, hidden = self.gru(embedded, hidden)\n","        prediction = self.softmax(self.out(output[0]))\n","        return prediction, hidden\n","\n"],"metadata":{"id":"FWI609UQrabH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seq2seq 네트워크"],"metadata":{"id":"MYVY9qV-60z3"}},{"cell_type":"code","source":["# Seq2seq 네트워크\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n","        super().__init__()\n","        # 인코더와 디코더 초기화\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n","\n","        input_length = input_lang.size(0) # 입력 문장 길이(문장 단어수)\n","        batch_size = output_lang.shape[1]\n","        target_length = output_lang.shape[0]\n","        vocab_size = self.decoder.output_dim\n","        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n","\n","        for i in range(input_length):\n","            # 문장의 모든 단어 인코딩\n","            encoder_output, encoder_hidden = self.encoder(input_lang[i])\n","\n","        # 인코더 은닉층 -> 디코더 은닉층\n","        decoder_hidden = encoder_hidden.to(device)\n","        # 예측 단어 앞에 SOS token 추가\n","        decoder_input = torch.tensor([SOS_token], device=device)\n","\n","        for t in range(target_length):\n","            # 현재 단어에서 출력단어 예측\n","            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n","            outputs[t] = decoder_output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            topv, topi = decoder_output.topk(1)\n","            # teacher force 활성화하면 모표를 다음 입력으로 사용\n","            input = (output_lang[t] if teacher_force else topi)\n","            # teacher force 활성화하지 않으면 자체 예측 값을 다음 입력으로 사용\n","            if (teacher_force == False and input.item() == EOS_token) :\n","                break\n","        return outputs\n","# teacher_force : seq2seq에서 많이 사용되는 기법. 번역(예측)하려는 목표 단어를 디코더의 다음 입력으로 넣어줌"],"metadata":{"id":"KNgJg3CgraeC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델의 오차 계산 함수, 훈련함수 정의"],"metadata":{"id":"hh_bYqRTGiRM"}},{"cell_type":"code","source":["# 모델의 오차 계산 함수 정의\n","teacher_forcing_ratio = 0.5\n","\n","def Model(model, input_tensor, target_tensor, model_optimizer, criterion):\n","    model_optimizer.zero_grad()\n","    input_length = input_tensor.size(0)\n","    loss = 0\n","    epoch_loss = 0\n","    output = model(input_tensor, target_tensor)\n","    num_iter = output.size(0)\n","\n","    for ot in range(num_iter):\n","        loss += criterion(output[ot], target_tensor[ot])\n","\n","    loss.backward()\n","    model_optimizer.step()\n","    epoch_loss = loss.item() / num_iter\n","    return epoch_loss"],"metadata":{"id":"JGGnBostraj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 훈련함수 정의\n","# 모델 훈련 함수, 옵티마이저, 손실 함수 정의\n","def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n","    model.train()\n","    optimizer = optim.SGD(model.parameters(), lr=0.01)\n","    criterion = nn.NLLLoss()\n","    total_loss_iterations = 0\n","\n","    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n","                      for i in range(num_iteration)]\n","\n","    for iter in range(1, num_iteration+1):\n","        training_pair = training_pairs[iter - 1]\n","        input_tensor = training_pair[0]\n","        target_tensor = training_pair[1]\n","        loss = Model(model, input_tensor, target_tensor, optimizer, criterion)\n","        total_loss_iterations += loss\n","\n","        if iter % 5000 == 0:\n","            average_loss= total_loss_iterations / 5000\n","            total_loss_iterations = 0\n","            print('%d %.4f' % (iter, average_loss))\n","\n","    torch.save(model.state_dict(), './mytraining.pt')\n","    return model"],"metadata":{"id":"b-jBXKIWrwhu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 모델 평가 및 훈련"],"metadata":{"id":"56uVeH-sGvQR"}},{"cell_type":"code","source":["# 모델 평가\n","def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n","    with torch.no_grad():\n","        input_tensor = tensorFromSentence(input_lang, sentences[0])\n","        output_tensor = tensorFromSentence(output_lang, sentences[1])\n","        decoded_words = []\n","        output = model(input_tensor, output_tensor)\n","\n","        for ot in range(output.size(0)):\n","            topv, topi = output[ot].topk(1)\n","\n","            if topi[0].item() == EOS_token:\n","                decoded_words.append('<EOS>')\n","                break\n","            else:\n","                decoded_words.append(output_lang.index2word[topi[0].item()])\n","    return decoded_words\n","\n","def evaluateRandomly(model, input_lang, output_lang, pairs, n=10):\n","    for i in range(n):\n","        pair = random.choice(pairs)\n","        print('input {}'.format(pair[0]))\n","        print('output {}'.format(pair[1]))\n","        output_words = evaluate(model, input_lang, output_lang, pair)\n","        output_sentence = ' '.join(output_words)\n","        print('predicted {}'.format(output_sentence))"],"metadata":{"id":"0W_x1RXorwkn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 훈련\n","lang1 = 'eng'\n","lang2 = 'fra'\n","input_lang, output_lang, pairs = process_data(lang1, lang2)\n","\n","randomize = random.choice(pairs)\n","print('random sentence {}'.format(randomize))\n","\n","input_size = input_lang.n_words\n","output_size = output_lang.n_words\n","print('Input : {} Output : {}'.format(input_size, output_size))\n","\n","embed_size = 256\n","hidden_size = 512\n","num_layers = 1\n","num_iteration = 75000\n","\n","encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n","decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n","\n","model = Seq2Seq(encoder, decoder, device).to(device)\n","\n","print(encoder)\n","print(decoder)\n","\n","model = trainModel(model, input_lang, output_lang, pairs, num_iteration)"],"metadata":{"id":"82t4UiKGrwnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 예측 결과 출력"],"metadata":{"id":"BmubU_LzG1QT"}},{"cell_type":"code","source":["# 랜덤 예측 결과 출력\n","evaluateRandomly(model, input_lang, output_lang, pairs)"],"metadata":{"id":"bDGrd28Nrwp_"},"execution_count":null,"outputs":[]}]}