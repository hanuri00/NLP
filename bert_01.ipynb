{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1692930992302,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"-kw83nRkjIZ_"},"outputs":[],"source":["#bert"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2576,"status":"ok","timestamp":1692931889626,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"J8_uIm5NlkJu","outputId":"52a0067f-0614-4944-d096-6d31d1c87991"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1692932141321,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"eUye5jIxnk6X","outputId":"6b379b31-2b7d-4779-fdc7-1cd93f6ce856"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16597,"status":"ok","timestamp":1692931008896,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"twIuIlfsjM4j","outputId":"9d1fd45f-9343-4f73-fe20-960d3b5d3f32"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub\u003c1.0,\u003e=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,\u003c0.14,\u003e=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors\u003e=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.15.1-\u003etransformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.15.1-\u003etransformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18796,"status":"ok","timestamp":1692931033785,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"i1vP3YSfjNtY","outputId":"5c99877a-2c76-4e91-e38d-13ce45a10e7c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch-transformers\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/176.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.0.1+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (1.23.5)\n","Collecting boto3 (from pytorch-transformers)\n","  Downloading boto3-1.28.34-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (4.66.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-transformers) (2023.6.3)\n","Collecting sentencepiece (from pytorch-transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacremoses (from pytorch-transformers)\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.0.0-\u003epytorch-transformers) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.0.0-\u003epytorch-transformers) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.0.0-\u003epytorch-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.0.0-\u003epytorch-transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.0.0-\u003epytorch-transformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch\u003e=1.0.0-\u003epytorch-transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.0.0-\u003epytorch-transformers) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0-\u003etorch\u003e=1.0.0-\u003epytorch-transformers) (16.0.6)\n","Collecting botocore\u003c1.32.0,\u003e=1.31.34 (from boto3-\u003epytorch-transformers)\n","  Downloading botocore-1.31.34-py3-none-any.whl (11.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath\u003c2.0.0,\u003e=0.7.1 (from boto3-\u003epytorch-transformers)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer\u003c0.7.0,\u003e=0.6.0 (from boto3-\u003epytorch-transformers)\n","  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003epytorch-transformers) (3.2.0)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003epytorch-transformers) (3.4)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003epytorch-transformers) (2.0.4)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003epytorch-transformers) (2023.7.22)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses-\u003epytorch-transformers) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses-\u003epytorch-transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses-\u003epytorch-transformers) (1.3.2)\n","Requirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore\u003c1.32.0,\u003e=1.31.34-\u003eboto3-\u003epytorch-transformers) (2.8.2)\n","Collecting urllib3\u003c3,\u003e=1.21.1 (from requests-\u003epytorch-transformers)\n","  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003etorch\u003e=1.0.0-\u003epytorch-transformers) (2.1.3)\n","Requirement already satisfied: mpmath\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy-\u003etorch\u003e=1.0.0-\u003epytorch-transformers) (1.3.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895240 sha256=e21f7dbf6a1ce96da1ea6ae4c16e9926d28e637dbdda4ad53a52793d45f20a5f\n","  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, urllib3, sacremoses, jmespath, botocore, s3transfer, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.4\n","    Uninstalling urllib3-2.0.4:\n","      Successfully uninstalled urllib3-2.0.4\n","Successfully installed boto3-1.28.34 botocore-1.31.34 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.6.2 sacremoses-0.0.53 sentencepiece-0.1.99 urllib3-1.26.16\n"]}],"source":["!pip install pytorch-transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":6712,"status":"ok","timestamp":1692931202655,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"2a93_QqOjT37"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from pytorch_transformers import BertTokenizer, BertForSequenceClassification\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import seaborn as sns\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1952,"status":"ok","timestamp":1692932249139,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"7tFaPs6Ij_9g"},"outputs":[],"source":["train_df = pd.read_csv('/content/drive/MyDrive/NLP/training.txt', sep='\\t')   #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n","valid_df = pd.read_csv('/content/drive/MyDrive/NLP/validing.txt', sep='\\t')   #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n","test_df = pd.read_csv('/content/drive/MyDrive/NLP/testing.txt', sep='\\t')"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":370,"status":"ok","timestamp":1692932269066,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"Yy-v0TrGkwDA"},"outputs":[],"source":["train_df = train_df.sample(frac=0.1, random_state=500)\n","valid_df = valid_df.sample(frac=0.1, random_state=500)\n","test_df = test_df.sample(frac=0.1, random_state=500)"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692932276181,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"lRGnt3u6oF9v"},"outputs":[],"source":["class Datasets(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        text = self.df.iloc[idx, 1]\n","        label = self.df.iloc[idx, 2]\n","        return text, label"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692932284210,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"BhZqO8xjoHsg"},"outputs":[],"source":["train_dataset = Datasets(train_df)\n","train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n","\n","valid_dataset = Datasets(valid_df)\n","valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers=0)\n","\n","test_dataset = Datasets(test_df)\n","test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, num_workers=0)"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24667,"status":"ok","timestamp":1692932315794,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"XJsVZIJ6oJrd","outputId":"ee4ec283-10eb-4167-9aa3-941a0e1b2e61"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 231508/231508 [00:00\u003c00:00, 1375363.57B/s]\n","100%|██████████| 433/433 [00:00\u003c00:00, 297531.72B/s]\n","100%|██████████| 440473133/440473133 [00:10\u003c00:00, 40148270.31B/s]\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n","model.to(device)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692932354778,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"DYFnvQhIoLf4"},"outputs":[],"source":["def save_checkpoint(save_path, model, valid_loss):\n","    if save_path == None:\n","        return\n","    state_dict = {'model_state_dict': model.state_dict(),\n","                  'valid_loss': valid_loss}\n","\n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==\u003e {save_path}')\n","\n","def load_checkpoint(load_path, model):\n","    if load_path==None:\n","        return\n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from \u003c== {load_path}')\n","\n","    model.load_state_dict(state_dict['model_state_dict'])\n","    return state_dict['valid_loss']\n","\n","def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n","    if save_path == None:\n","        return\n","    state_dict = {'train_loss_list': train_loss_list,\n","                  'valid_loss_list': valid_loss_list,\n","                  'global_steps_list': global_steps_list}\n","    torch.save(state_dict, save_path)\n","    print(f'Model saved to ==\u003e {save_path}')\n","\n","def load_metrics(load_path):\n","    if load_path==None:\n","        return\n","    state_dict = torch.load(load_path, map_location=device)\n","    print(f'Model loaded from \u003c== {load_path}')\n","    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":385,"status":"ok","timestamp":1692932368192,"user":{"displayName":"SukJIn Kim","userId":"17812388049255700295"},"user_tz":-540},"id":"LE9y4qxqoa44"},"outputs":[],"source":["def train(model,\n","          optimizer,\n","          criterion = nn.BCELoss(),\n","          num_epochs = 5,\n","          eval_every = len(train_loader) // 2,\n","          best_valid_loss = float(\"Inf\")):\n","\n","    total_correct = 0.0\n","    total_len = 0.0\n","    running_loss = 0.0\n","    valid_running_loss = 0.0\n","    global_step = 0\n","    train_loss_list = []\n","    valid_loss_list = []\n","    global_steps_list = []\n","\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for text, label in train_loader:\n","            optimizer.zero_grad()\n","            encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n","\n","            sample = torch.tensor(padded_list)\n","            sample, label = sample.to(device), label.to(device)\n","            labels = torch.tensor(label)\n","            outputs = model(sample, labels=labels)\n","            loss, logits = outputs\n","\n","            pred = torch.argmax(F.softmax(logits), dim=1)\n","            correct = pred.eq(labels)\n","            total_correct += correct.sum().item()\n","            total_len += len(labels)\n","            running_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","            global_step += 1\n","\n","            if global_step % eval_every == 0:\n","                model.eval()\n","                with torch.no_grad():\n","                    for text, label in valid_loader:\n","                        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n","                        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n","                        sample = torch.tensor(padded_list)\n","                        sample, label = sample.to(device), label.to(device)\n","                        labels = torch.tensor(label)\n","                        outputs = model(sample, labels=labels)\n","                        loss, logits = outputs\n","                        valid_running_loss += loss.item()\n","\n","                average_train_loss = running_loss / eval_every\n","                average_valid_loss = valid_running_loss / len(valid_loader)\n","                train_loss_list.append(average_train_loss)\n","                valid_loss_list.append(average_valid_loss)\n","                global_steps_list.append(global_step)\n","\n","                running_loss = 0.0\n","                valid_running_loss = 0.0\n","                model.train()\n","\n","                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n","                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n","                              average_train_loss, average_valid_loss))\n","\n","                if best_valid_loss \u003e average_valid_loss:\n","                    best_valid_loss = average_valid_loss\n","                    save_checkpoint('/content/drive/MyDrive/Colab Notebooks/model.pt', model, best_valid_loss)\n","                    save_metrics('/content/drive/MyDrive/Colab Notebooks/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n","\n","    save_metrics('/content/drive/MyDrive/Colab Notebooks/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n","    print('훈련 종료!')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"CDmicUMXoeIF"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u003cipython-input-19-6f3a5ade20c4\u003e:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  labels = torch.tensor(label)\n","\u003cipython-input-19-6f3a5ade20c4\u003e:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  pred = torch.argmax(F.softmax(logits), dim=1)\n"]}],"source":["optimizer = optim.Adam(model.parameters(), lr=2e-5)\n","train(model=model, optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ey2mxlr1oinY"},"outputs":[],"source":["train_loss_list, valid_loss_list, global_steps_list = load_metrics('/content/drive/MyDrive/Colab Notebooks/metrics.pt')\n","plt.plot(global_steps_list, train_loss_list, label='Train')\n","plt.plot(global_steps_list, valid_loss_list, label='Valid')\n","plt.xlabel('Global Steps')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPzppDZ2nl0arljmWAzUnHS","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}