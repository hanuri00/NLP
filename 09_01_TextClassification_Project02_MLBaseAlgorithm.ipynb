{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanuri00/NLP/blob/main/09_01_TextClassification_Project02_MLBaseAlgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N0PU6VLAZzn"
      },
      "source": [
        "# 09주 1차시 프로젝트02 머신러닝 기반 텍스트분류- 기초 알고리즘\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "wwmTT9kkBpYa",
        "outputId": "fd6a7636-4001-41d9-a25d-3f3d3512f4a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ef2e31d4-3ad9-4895-ad09-6b361d9d607f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ef2e31d4-3ad9-4895-ad09-6b361d9d607f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving class2.csv to class2.csv\n"
          ]
        }
      ],
      "source": [
        "#데이터 준비\n",
        "from google.colab import files # 데이터 불러오기\n",
        "fiel_upload = files.upload()      #class2.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPzyyfKpBqk8",
        "outputId": "481e0afd-fe90-42c9-fc2c-73fb799875fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 2, 1, 0, 1, 0])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#희소표현(Sparse Representation)\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "class2=pd.read_csv(\"class2.csv\")\n",
        "\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "onehot_encoder = preprocessing.OneHotEncoder()\n",
        "\n",
        "train_x = label_encoder.fit_transform(class2['class2'])\n",
        "train_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoNRB3iwBqrQ",
        "outputId": "935a2bca-171a-4641-efa0-ea909b101ead"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'this': 13,\n",
              " 'is': 7,\n",
              " 'last': 8,\n",
              " 'chance': 2,\n",
              " 'and': 0,\n",
              " 'if': 6,\n",
              " 'you': 15,\n",
              " 'do': 3,\n",
              " 'not': 10,\n",
              " 'have': 5,\n",
              " 'will': 14,\n",
              " 'never': 9,\n",
              " 'get': 4,\n",
              " 'any': 1,\n",
              " 'one': 11,\n",
              " 'please': 12}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Counter Vector\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is last chance.',\n",
        "    'and if you do not have this chance.',\n",
        "    'you will never get any chance.',\n",
        "    'will you do get this one?',\n",
        "    'please, get this chance',\n",
        "]\n",
        "vect = CountVectorizer()\n",
        "vect.fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Az33wnAIH9h",
        "outputId": "2f9f8ce4-fb68-432c-c894-b437531fea35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vect.transform(['you will never get any chance.']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXWui1oBIKnp",
        "outputId": "f9cbf794-0bc0-476d-8a86-9e17100d7cda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'last': 6,\n",
              " 'chance': 1,\n",
              " 'if': 5,\n",
              " 'you': 11,\n",
              " 'do': 2,\n",
              " 'not': 8,\n",
              " 'have': 4,\n",
              " 'will': 10,\n",
              " 'never': 7,\n",
              " 'get': 3,\n",
              " 'any': 0,\n",
              " 'one': 9}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"please\", \"this\"]).fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0b7q1mQIMet",
        "outputId": "834563a4-7884-4c43-8ec3-5a7f23aa40c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "유사도를 위한 3 x 3 matrix를 만들었습니다.\n",
            "[[1.       0.224325 0.      ]\n",
            " [0.224325 1.       0.      ]\n",
            " [0.       0.       1.      ]]\n"
          ]
        }
      ],
      "source": [
        "#TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc = ['I like machine learning', 'I love deep learning', 'I run everyday']\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
        "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
        "print ('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), 'matrix를 만들었습니다.')\n",
        "print(doc_distance.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdpRpUPvIQ3f",
        "outputId": "1dc75900-134f-4b27-d7de-fee6de2a21fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yarwzhvfIVKJ",
        "outputId": "bcef0a66-1464-4bd8-9e6c-ae284143d785"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        },
        "id": "hGgfSFJNIZHV",
        "outputId": "692066d5-78e0-4e9b-99f7-623c4a3345f4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-90cfcc36-4668-450f-8757-46f3ea739402\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-90cfcc36-4668-450f-8757-46f3ea739402\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving peter.txt to peter.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files # 데이터 불러오기\n",
        "file_uploaded=files.upload()   # peter.txt 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1cEyWcZJEWX",
        "outputId": "cb4d3e0c-e3a1-4414-a1d4-aeb66bf785dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['once',\n",
              "  'upon',\n",
              "  'a',\n",
              "  'time',\n",
              "  'in',\n",
              "  'london',\n",
              "  ',',\n",
              "  'the',\n",
              "  'darlings',\n",
              "  'went',\n",
              "  'out',\n",
              "  'to',\n",
              "  'a',\n",
              "  'dinner',\n",
              "  'party',\n",
              "  'leaving',\n",
              "  'their',\n",
              "  'three',\n",
              "  'children',\n",
              "  'wendy',\n",
              "  ',',\n",
              "  'jhon',\n",
              "  ',',\n",
              "  'and',\n",
              "  'michael',\n",
              "  'at',\n",
              "  'home',\n",
              "  '.'],\n",
              " ['after',\n",
              "  'wendy',\n",
              "  'had',\n",
              "  'tucked',\n",
              "  'her',\n",
              "  'younger',\n",
              "  'brothers',\n",
              "  'jhon',\n",
              "  'and',\n",
              "  'michael',\n",
              "  'to',\n",
              "  'bed',\n",
              "  ',',\n",
              "  'she',\n",
              "  'went',\n",
              "  'to',\n",
              "  'read',\n",
              "  'a',\n",
              "  'book',\n",
              "  '.'],\n",
              " ['she', 'heard', 'a', 'boy', 'sobbing', 'outside', 'her', 'window', '.'],\n",
              " ['he', 'was', 'flying', '.'],\n",
              " ['there', 'was', 'little', 'fairy', 'fluttering', 'around', 'him', '.'],\n",
              " ['wendy', 'opened', 'the', 'window', 'to', 'talk', 'to', 'him', '.'],\n",
              " ['“', 'hello', '!'],\n",
              " ['who', 'are', 'you', '?'],\n",
              " ['why', 'are', 'you', 'crying', '”', ',', 'wendy', 'asked', 'him', '.'],\n",
              " ['“', 'my', 'name', 'is', 'peter', 'pan', '.'],\n",
              " ['my',\n",
              "  'shadow',\n",
              "  'wouldn',\n",
              "  '’',\n",
              "  't',\n",
              "  'stock',\n",
              "  'to',\n",
              "  'me.',\n",
              "  '”',\n",
              "  ',',\n",
              "  'he',\n",
              "  'replied',\n",
              "  '.'],\n",
              " ['she', 'asked', 'him', 'to', 'come', 'in', '.'],\n",
              " ['peter', 'agreed', 'and', 'came', 'inside', 'the', 'room', '.'],\n",
              " ['wendy',\n",
              "  'took',\n",
              "  'his',\n",
              "  'shadow',\n",
              "  'and',\n",
              "  'sewed',\n",
              "  'it',\n",
              "  'to',\n",
              "  'his',\n",
              "  'shoe',\n",
              "  'tips',\n",
              "  '.'],\n",
              " ['now',\n",
              "  'his',\n",
              "  'shadow',\n",
              "  'followed',\n",
              "  'him',\n",
              "  'wherever',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'went',\n",
              "  '!'],\n",
              " ['he',\n",
              "  'was',\n",
              "  'delighted',\n",
              "  'and',\n",
              "  'asked',\n",
              "  'wendy',\n",
              "  '“',\n",
              "  'why',\n",
              "  'don',\n",
              "  '’',\n",
              "  't',\n",
              "  'you',\n",
              "  'come',\n",
              "  'with',\n",
              "  'me',\n",
              "  'to',\n",
              "  'my',\n",
              "  'home',\n",
              "  '.'],\n",
              " ['the', 'neverland', '.'],\n",
              " ['i',\n",
              "  'lived',\n",
              "  'there',\n",
              "  'with',\n",
              "  'my',\n",
              "  'fairy',\n",
              "  'tinker',\n",
              "  'bell.',\n",
              "  '”',\n",
              "  'wendy',\n",
              "  '?'],\n",
              " ['“', 'oh', '!'],\n",
              " ['what', 'a', 'wonderful', 'idea', '!'],\n",
              " ['let', 'me', 'wake', 'up', 'john', 'and', 'micheal', 'too', '.'],\n",
              " ['could', 'you', 'teach', 'us', 'how', 'to', 'fly', '?', '”', '.'],\n",
              " ['“', 'yes', '!'],\n",
              " ['of', 'course', '!'],\n",
              " ['get',\n",
              "  'them',\n",
              "  'we',\n",
              "  'will',\n",
              "  'all',\n",
              "  'fly',\n",
              "  'together.',\n",
              "  '”',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'replied',\n",
              "  'and',\n",
              "  'so',\n",
              "  'it',\n",
              "  'was',\n",
              "  '.'],\n",
              " ['five',\n",
              "  'little',\n",
              "  'figures',\n",
              "  'flew',\n",
              "  'out',\n",
              "  'of',\n",
              "  'the',\n",
              "  'window',\n",
              "  'of',\n",
              "  'the',\n",
              "  'darlings',\n",
              "  'and',\n",
              "  'headed',\n",
              "  'towards',\n",
              "  'neverland',\n",
              "  '.'],\n",
              " ['as',\n",
              "  'they',\n",
              "  'flew',\n",
              "  'over',\n",
              "  'the',\n",
              "  'island',\n",
              "  ',',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'told',\n",
              "  'the',\n",
              "  'children',\n",
              "  'more',\n",
              "  'about',\n",
              "  'his',\n",
              "  'homeland',\n",
              "  '.'],\n",
              " ['“',\n",
              "  'all',\n",
              "  'the',\n",
              "  'children',\n",
              "  'who',\n",
              "  'get',\n",
              "  'lost',\n",
              "  'come',\n",
              "  'and',\n",
              "  'stay',\n",
              "  'with',\n",
              "  'tinker',\n",
              "  'bell',\n",
              "  'and',\n",
              "  'me',\n",
              "  ',',\n",
              "  '”',\n",
              "  'peter',\n",
              "  'told',\n",
              "  'them',\n",
              "  '.'],\n",
              " ['the', 'indians', 'also', 'live', 'in', 'neverland', '.'],\n",
              " ['the',\n",
              "  'mermaids',\n",
              "  'live',\n",
              "  'in',\n",
              "  'the',\n",
              "  'lagoon',\n",
              "  'around',\n",
              "  'the',\n",
              "  'island',\n",
              "  '.'],\n",
              " ['and',\n",
              "  'a',\n",
              "  'very',\n",
              "  'mean',\n",
              "  'pirate',\n",
              "  'called',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  'keeps',\n",
              "  'troubling',\n",
              "  'everyone',\n",
              "  '.'],\n",
              " ['“', 'crocodile', 'bit', 'his', 'one', 'arm', '.'],\n",
              " ['so',\n",
              "  'the',\n",
              "  'captain',\n",
              "  'had',\n",
              "  'to',\n",
              "  'put',\n",
              "  'a',\n",
              "  'hook',\n",
              "  'in',\n",
              "  'its',\n",
              "  'place',\n",
              "  '.'],\n",
              " ['since', 'then', 'he', 'is', 'afraid', 'of', 'crocodiles', '.'],\n",
              " ['and', 'rightly', 'so', '!'],\n",
              " ['if',\n",
              "  'the',\n",
              "  'crocodile',\n",
              "  'ever',\n",
              "  'found',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  'it',\n",
              "  'will',\n",
              "  'eat',\n",
              "  'up',\n",
              "  'the',\n",
              "  'rest',\n",
              "  'of',\n",
              "  'it',\n",
              "  'couldn',\n",
              "  '’',\n",
              "  't',\n",
              "  'eat',\n",
              "  'last',\n",
              "  'time.',\n",
              "  '”',\n",
              "  'peter',\n",
              "  'told',\n",
              "  'them',\n",
              "  '.'],\n",
              " ['soon', 'they', 'landed', 'on', 'the', 'island', '.'],\n",
              " ['and',\n",
              "  'to',\n",
              "  'the',\n",
              "  'surprise',\n",
              "  'of',\n",
              "  'wendy',\n",
              "  ',',\n",
              "  'jhon',\n",
              "  'and',\n",
              "  'michael',\n",
              "  ',',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'let',\n",
              "  'them',\n",
              "  'in',\n",
              "  'through',\n",
              "  'a',\n",
              "  'small',\n",
              "  'opening',\n",
              "  'in',\n",
              "  'a',\n",
              "  'tree',\n",
              "  '.'],\n",
              " ['inside',\n",
              "  'the',\n",
              "  'tree',\n",
              "  'was',\n",
              "  'a',\n",
              "  'large',\n",
              "  'room',\n",
              "  'with',\n",
              "  'children',\n",
              "  'inside',\n",
              "  'it',\n",
              "  '.'],\n",
              " ['somewhere',\n",
              "  'huddled',\n",
              "  'by',\n",
              "  'the',\n",
              "  'fire',\n",
              "  'in',\n",
              "  'the',\n",
              "  'corner',\n",
              "  'and',\n",
              "  'somewhere',\n",
              "  'playing',\n",
              "  'amongst',\n",
              "  'themselves',\n",
              "  '.'],\n",
              " ['their',\n",
              "  'faces',\n",
              "  'lit',\n",
              "  'up',\n",
              "  'when',\n",
              "  'they',\n",
              "  'saw',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  ',',\n",
              "  'tinker',\n",
              "  'bell',\n",
              "  ',',\n",
              "  'and',\n",
              "  'their',\n",
              "  'guests',\n",
              "  '.'],\n",
              " ['“', 'hello', 'everyone', '.'],\n",
              " ['this', 'is', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
              " ['they',\n",
              "  'will',\n",
              "  'be',\n",
              "  'staying',\n",
              "  'with',\n",
              "  'us',\n",
              "  'from',\n",
              "  'now',\n",
              "  'on.',\n",
              "  '”',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'introduced',\n",
              "  'them',\n",
              "  'to',\n",
              "  'all',\n",
              "  'children',\n",
              "  '.'],\n",
              " ['children', 'welcomed', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
              " ['a', 'few', 'days', 'passed', '.'],\n",
              " ['and', 'they', 'settled', 'into', 'a', 'routine', '.'],\n",
              " ['wendy',\n",
              "  'would',\n",
              "  'take',\n",
              "  'care',\n",
              "  'of',\n",
              "  'all',\n",
              "  'the',\n",
              "  'children',\n",
              "  'in',\n",
              "  'the',\n",
              "  'day',\n",
              "  'and',\n",
              "  'would',\n",
              "  'go',\n",
              "  'out',\n",
              "  'with',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'and',\n",
              "  'her',\n",
              "  'brothers',\n",
              "  'in',\n",
              "  'the',\n",
              "  'evening',\n",
              "  'to',\n",
              "  'learn',\n",
              "  'about',\n",
              "  'the',\n",
              "  'island',\n",
              "  '.'],\n",
              " ['she',\n",
              "  'would',\n",
              "  'cook',\n",
              "  'for',\n",
              "  'them',\n",
              "  'and',\n",
              "  'stitch',\n",
              "  'new',\n",
              "  'clothes',\n",
              "  'for',\n",
              "  'them',\n",
              "  '.'],\n",
              " ['he',\n",
              "  'even',\n",
              "  'made',\n",
              "  'a',\n",
              "  'lovely',\n",
              "  'new',\n",
              "  'dress',\n",
              "  'for',\n",
              "  'tinker',\n",
              "  'bell',\n",
              "  '.'],\n",
              " ['one',\n",
              "  'evening',\n",
              "  ',',\n",
              "  'as',\n",
              "  'they',\n",
              "  'were',\n",
              "  'out',\n",
              "  'exploring',\n",
              "  'the',\n",
              "  'island',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'warned',\n",
              "  'everyone',\n",
              "  'and',\n",
              "  'said',\n",
              "  ',',\n",
              "  '“',\n",
              "  'hide',\n",
              "  '!'],\n",
              " ['hide', '!'],\n",
              " ['pirates', '!'],\n",
              " ['and',\n",
              "  'they',\n",
              "  'have',\n",
              "  'kidnapped',\n",
              "  'the',\n",
              "  'indian',\n",
              "  'princess',\n",
              "  'tiger',\n",
              "  'lily',\n",
              "  '.'],\n",
              " ['they',\n",
              "  'have',\n",
              "  'kept',\n",
              "  'her',\n",
              "  'there',\n",
              "  ',',\n",
              "  'tied',\n",
              "  'up',\n",
              "  'by',\n",
              "  'the',\n",
              "  'rocks',\n",
              "  ',',\n",
              "  'near',\n",
              "  'the',\n",
              "  'water.',\n",
              "  '”',\n",
              "  'peter',\n",
              "  'was',\n",
              "  'afraid',\n",
              "  'and',\n",
              "  'the',\n",
              "  'princess',\n",
              "  'would',\n",
              "  'drown',\n",
              "  ',',\n",
              "  'is',\n",
              "  'she',\n",
              "  'fell',\n",
              "  'into',\n",
              "  'the',\n",
              "  'water',\n",
              "  '.'],\n",
              " ['so',\n",
              "  ',',\n",
              "  'in',\n",
              "  'a',\n",
              "  'voice',\n",
              "  'that',\n",
              "  'sounded',\n",
              "  'like',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  ',',\n",
              "  'he',\n",
              "  'shouted',\n",
              "  'instructions',\n",
              "  'to',\n",
              "  'the',\n",
              "  'pirates',\n",
              "  'who',\n",
              "  'guarded',\n",
              "  'her',\n",
              "  ',',\n",
              "  '“',\n",
              "  'you',\n",
              "  'fools',\n",
              "  '!'],\n",
              " ['let', 'her', 'go', 'at', 'once', '!'],\n",
              " ['do',\n",
              "  'it',\n",
              "  'before',\n",
              "  'i',\n",
              "  'come',\n",
              "  'there',\n",
              "  'or',\n",
              "  'else',\n",
              "  'i',\n",
              "  'will',\n",
              "  'throw',\n",
              "  'each',\n",
              "  'one',\n",
              "  'of',\n",
              "  'you',\n",
              "  'into',\n",
              "  'the',\n",
              "  'water.',\n",
              "  '”',\n",
              "  'the',\n",
              "  'pirates',\n",
              "  'got',\n",
              "  'scared',\n",
              "  'and',\n",
              "  'immediately',\n",
              "  'released',\n",
              "  'the',\n",
              "  'princes',\n",
              "  '.'],\n",
              " ['she',\n",
              "  'quickly',\n",
              "  'dived',\n",
              "  'into',\n",
              "  'the',\n",
              "  'water',\n",
              "  'and',\n",
              "  'swam',\n",
              "  'to',\n",
              "  'the',\n",
              "  'safety',\n",
              "  'of',\n",
              "  'her',\n",
              "  'home',\n",
              "  '.'],\n",
              " ['soon',\n",
              "  'everyone',\n",
              "  'found',\n",
              "  'out',\n",
              "  'how',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'had',\n",
              "  'rescued',\n",
              "  'the',\n",
              "  'princess',\n",
              "  '.'],\n",
              " ['when',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  'found',\n",
              "  'out',\n",
              "  'how',\n",
              "  'peter',\n",
              "  'had',\n",
              "  'tricked',\n",
              "  'his',\n",
              "  'men',\n",
              "  'he',\n",
              "  'was',\n",
              "  'furious',\n",
              "  '.'],\n",
              " ['and', 'swore', 'to', 'have', 'his', 'revenge', '.'],\n",
              " ['that',\n",
              "  'night',\n",
              "  'wendy',\n",
              "  'told',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  ',',\n",
              "  'that',\n",
              "  'she',\n",
              "  'and',\n",
              "  'her',\n",
              "  'brother',\n",
              "  'wanted',\n",
              "  'to',\n",
              "  'go',\n",
              "  'back',\n",
              "  'home',\n",
              "  'since',\n",
              "  'they',\n",
              "  'missed',\n",
              "  'their',\n",
              "  'parents',\n",
              "  '.'],\n",
              " ['she',\n",
              "  'said',\n",
              "  'if',\n",
              "  'the',\n",
              "  'lost',\n",
              "  'children',\n",
              "  'could',\n",
              "  'also',\n",
              "  'return',\n",
              "  'to',\n",
              "  'her',\n",
              "  'world',\n",
              "  'they',\n",
              "  'could',\n",
              "  'find',\n",
              "  'a',\n",
              "  'nice',\n",
              "  'home',\n",
              "  'for',\n",
              "  'them',\n",
              "  '.'],\n",
              " ['peter', 'pan', 'didn', '’', 't', 'want', 'to', 'leave', 'neverland', '.'],\n",
              " ['but',\n",
              "  'the',\n",
              "  'sake',\n",
              "  'of',\n",
              "  'the',\n",
              "  'lost',\n",
              "  'children',\n",
              "  'he',\n",
              "  'agreed',\n",
              "  ',',\n",
              "  'although',\n",
              "  'a',\n",
              "  'bit',\n",
              "  'sadly',\n",
              "  '.'],\n",
              " ['he', 'would', 'miss', 'his', 'friends', 'dearly', '.'],\n",
              " ['the',\n",
              "  'next',\n",
              "  'morning',\n",
              "  'all',\n",
              "  'the',\n",
              "  'lost',\n",
              "  'children',\n",
              "  'left',\n",
              "  'with',\n",
              "  'wendy',\n",
              "  ',',\n",
              "  'jhon',\n",
              "  ',',\n",
              "  'and',\n",
              "  'michael',\n",
              "  '.'],\n",
              " ['but',\n",
              "  'on',\n",
              "  'the',\n",
              "  'way',\n",
              "  ',',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  'and',\n",
              "  'his',\n",
              "  'men',\n",
              "  'kidnapped',\n",
              "  'all',\n",
              "  'of',\n",
              "  'them',\n",
              "  '.'],\n",
              " ['he',\n",
              "  'tied',\n",
              "  'them',\n",
              "  'and',\n",
              "  'kept',\n",
              "  'them',\n",
              "  'on',\n",
              "  'once',\n",
              "  'of',\n",
              "  'his',\n",
              "  'ships',\n",
              "  '.'],\n",
              " ['as',\n",
              "  'soon',\n",
              "  'as',\n",
              "  'peter',\n",
              "  'found',\n",
              "  'out',\n",
              "  'about',\n",
              "  'it',\n",
              "  'he',\n",
              "  'rushed',\n",
              "  'to',\n",
              "  'the',\n",
              "  'ship',\n",
              "  '.'],\n",
              " ['he',\n",
              "  'swung',\n",
              "  'himself',\n",
              "  'from',\n",
              "  'a',\n",
              "  'tress',\n",
              "  'branch',\n",
              "  'and',\n",
              "  'on',\n",
              "  'to',\n",
              "  'the',\n",
              "  'deck',\n",
              "  'of',\n",
              "  'the',\n",
              "  'ship',\n",
              "  'where',\n",
              "  'all',\n",
              "  'the',\n",
              "  'children',\n",
              "  'were',\n",
              "  'tied',\n",
              "  'up',\n",
              "  '.'],\n",
              " ['he',\n",
              "  'swung',\n",
              "  'his',\n",
              "  'sword',\n",
              "  'bravely',\n",
              "  'and',\n",
              "  'threw',\n",
              "  'over',\n",
              "  'the',\n",
              "  'pirates',\n",
              "  'who',\n",
              "  'tried',\n",
              "  'to',\n",
              "  'stop',\n",
              "  'him',\n",
              "  '.'],\n",
              " ['quickly',\n",
              "  'he',\n",
              "  'released',\n",
              "  'everyone',\n",
              "  'from',\n",
              "  'their',\n",
              "  'captor',\n",
              "  '’',\n",
              "  's',\n",
              "  'ties',\n",
              "  '.'],\n",
              " ['wendy',\n",
              "  ',',\n",
              "  'jhon',\n",
              "  ',',\n",
              "  'michael',\n",
              "  'and',\n",
              "  'tinker',\n",
              "  'bell',\n",
              "  'helped',\n",
              "  'all',\n",
              "  'the',\n",
              "  'children',\n",
              "  'into',\n",
              "  'the',\n",
              "  'water',\n",
              "  ',',\n",
              "  'where',\n",
              "  'their',\n",
              "  'friends',\n",
              "  'from',\n",
              "  'the',\n",
              "  'indian',\n",
              "  'camp',\n",
              "  'were',\n",
              "  'ready',\n",
              "  'with',\n",
              "  'smaller',\n",
              "  'boats',\n",
              "  'to',\n",
              "  'take',\n",
              "  'them',\n",
              "  'to',\n",
              "  'safety',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'now',\n",
              "  'went',\n",
              "  'looking',\n",
              "  'for',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  '.'],\n",
              " ['“',\n",
              "  'let',\n",
              "  'us',\n",
              "  'finished',\n",
              "  'this',\n",
              "  'forever',\n",
              "  'mr.',\n",
              "  'hook',\n",
              "  '”',\n",
              "  ',',\n",
              "  'peter',\n",
              "  'challenged',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  '.'],\n",
              " ['“', 'yes', '!'],\n",
              " ['peter',\n",
              "  'pan',\n",
              "  ',',\n",
              "  'you',\n",
              "  'have',\n",
              "  'caused',\n",
              "  'me',\n",
              "  'enough',\n",
              "  'trouble',\n",
              "  '.'],\n",
              " ['it',\n",
              "  'is',\n",
              "  'time',\n",
              "  'that',\n",
              "  'we',\n",
              "  'finished',\n",
              "  'this.',\n",
              "  '”',\n",
              "  'hook',\n",
              "  'replied',\n",
              "  '.'],\n",
              " ['with',\n",
              "  'his',\n",
              "  'sword',\n",
              "  'drawn',\n",
              "  ',',\n",
              "  'he',\n",
              "  'raced',\n",
              "  'towards',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  '.'],\n",
              " ['quick',\n",
              "  'on',\n",
              "  'his',\n",
              "  'feet',\n",
              "  ',',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'stepped',\n",
              "  'aside',\n",
              "  'and',\n",
              "  'pushed',\n",
              "  'hook',\n",
              "  'inside',\n",
              "  'the',\n",
              "  'sea',\n",
              "  'where',\n",
              "  'the',\n",
              "  'crocodile',\n",
              "  'was',\n",
              "  'waiting',\n",
              "  'to',\n",
              "  'eat',\n",
              "  'the',\n",
              "  'rest',\n",
              "  'of',\n",
              "  'hook',\n",
              "  '.'],\n",
              " ['everyone',\n",
              "  'rejoiced',\n",
              "  'as',\n",
              "  'captain',\n",
              "  'hook',\n",
              "  'was',\n",
              "  'out',\n",
              "  'of',\n",
              "  'their',\n",
              "  'lives',\n",
              "  'forever',\n",
              "  '.'],\n",
              " ['everybody', 'headed', 'back', 'to', 'london', '.'],\n",
              " ['mr.', 'and', 'mrs', '.'],\n",
              " ['darling',\n",
              "  'was',\n",
              "  'so',\n",
              "  'happy',\n",
              "  'to',\n",
              "  'see',\n",
              "  'their',\n",
              "  'children',\n",
              "  'and',\n",
              "  'they',\n",
              "  'agreed',\n",
              "  'to',\n",
              "  'adopt',\n",
              "  'the',\n",
              "  'lost',\n",
              "  'children',\n",
              "  '.'],\n",
              " ['they',\n",
              "  'even',\n",
              "  'asked',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'to',\n",
              "  'come',\n",
              "  'and',\n",
              "  'live',\n",
              "  'with',\n",
              "  'them',\n",
              "  '.'],\n",
              " ['but',\n",
              "  'peter',\n",
              "  'pan',\n",
              "  'said',\n",
              "  ',',\n",
              "  'he',\n",
              "  'never',\n",
              "  'wanted',\n",
              "  'to',\n",
              "  'grow',\n",
              "  'up',\n",
              "  ',',\n",
              "  'so',\n",
              "  'he',\n",
              "  'and',\n",
              "  'tinker',\n",
              "  'bell',\n",
              "  'will',\n",
              "  'go',\n",
              "  'back',\n",
              "  'to',\n",
              "  'neverland',\n",
              "  '.'],\n",
              " ['peter',\n",
              "  'pan',\n",
              "  'promised',\n",
              "  'everyone',\n",
              "  'that',\n",
              "  'he',\n",
              "  'will',\n",
              "  'visit',\n",
              "  'again',\n",
              "  'sometime',\n",
              "  '!'],\n",
              " ['and',\n",
              "  'he',\n",
              "  'flew',\n",
              "  'out',\n",
              "  'of',\n",
              "  'the',\n",
              "  'window',\n",
              "  'with',\n",
              "  'tinker',\n",
              "  'bell',\n",
              "  'by',\n",
              "  'his',\n",
              "  'side',\n",
              "  '.']]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sample = open(\"peter.txt\", \"r\", encoding='UTF8')\n",
        "s = sample.read()\n",
        "\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f):\n",
        "    temp = []\n",
        "    for j in word_tokenize(i):\n",
        "        temp.append(j.lower())\n",
        "    data.append(temp)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQLaetPwJFZ6",
        "outputId": "867fe6e9-456b-4fbc-fc62-fbdf6981b93a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity between 'peter' wendy' - CBOW :  1.0\n"
          ]
        }
      ],
      "source": [
        "#CBOW(Continuous Bag of Words)\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,  vector_size = 100, window = 5, sg=0)    #sg=0 CBOW, sg=1 skip-gram\n",
        "print(\"Cosine similarity between 'peter' \" + \"wendy' - CBOW : \", model1.wv.similarity('wendy', 'wendy'))\n",
        "\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,  vector_size = 100, window = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nB-qUdqJYYf",
        "outputId": "a3f99c04-2277-4015-8720-ca8704748deb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity between 'peter' hook' - CBOW :  0.027709836\n"
          ]
        }
      ],
      "source": [
        "print(\"Cosine similarity between 'peter' \" + \"hook' - CBOW : \", model1.wv.similarity('peter', 'hook'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We-pCpomJZx_",
        "outputId": "8eb3df99-4742-486c-d7b7-f1ebbc2664ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine similarity between 'peter' wendy' - Skip Gram :  0.40088683\n"
          ]
        }
      ],
      "source": [
        "#Skip-gram\n",
        "#특정한 단어에서 문맥 단어 예측(CBOW 방식과 반대)\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1,  vector_size = 100, window = 5, sg=1)    #sg=0 CBOW, sg=1 skip-gram\n",
        "print(\"Cosine similarity between 'peter' \" + \"wendy' - Skip Gram : \", model2.wv.similarity('peter', 'wendy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "vUo33yN9N2xY",
        "outputId": "b4a18e6c-65d8-4d9f-85ef-60777f06d188"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-dd6611f74029>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#model.train(gen_words(stemmed), total_examples=model.corpus_count, epochs=model.epochs)  #버전에 따라 에러 발생 가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#model.wv.similarity('data', 'scientist')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         self.add_lifecycle_event(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_check_corpus_sanity\u001b[0;34m(self, corpus_iterable, corpus_file, passes)\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \"The corpus_iterable must be an iterable of lists of strings, got %r instead\" % corpus_iterable)\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpasses\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m   1507\u001b[0m                 f\"Using a generator as corpus_iterable can't support {passes} passes. Try a re-iterable sequence.\")\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Using a generator as corpus_iterable can't support 5 passes. Try a re-iterable sequence."
          ]
        }
      ],
      "source": [
        "#FastText\n",
        "import gensim\n",
        "from gensim.models.fasttext import FastText as ft_gensim\n",
        "stemmed = ['database', 'science', 'scientist', 'mgmt', 'microsoft', 'hire', 'develop', 'mentor', 'team', 'data', 'scientist', 'define', 'dataloader', 'scienc', 'priority', 'deep', 'understand', 'learn', 'goal', 'collabor', 'across', 'triple', 'group', 'set', 'team', 'shortterm', 'longterm', 'goal', 'act', 'strait', 'advisor', 'leadership', 'influenc', 'future', 'direct', 'strategy', 'define', 'partnership', 'align', 'effect', 'broad', 'analyt', 'effort', 'analyticsdata', 'team', 'drive', 'part', 'datadog', 'scienc', 'bi', 'common', 'disciplin', 'microsoftprior', 'experi', 'hire', 'manage', 'runner', 'team', 'data', 'scientist', 'busi', 'domain', 'experi', 'usage', 'analyt', 'must', 'experi', 'across', 'sever', 'relev', 'busi', 'domain', 'util', 'critic', 'think', 'skill', 'concept', 'complex', 'busi', 'problem', 'salt', 'use', 'advanc', 'analsis', 'large', 'scale', 'realworld', 'busi', 'data', 'set', 'candid', 'must', 'abl', 'independ', 'execut', 'analyt', 'project', 'help', 'intern', 'client', 'understand']\n",
        "def gen_words(stemmed):\n",
        "    yield stemmed\n",
        "\n",
        "model = ft_gensim(vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "model.build_vocab(gen_words(stemmed))\n",
        "\n",
        "model.train(gen_words(stemmed), total_examples=model.corpus_count, epochs=model.iter)\n",
        "#model.wv.similarity('data', 'scientist')\n",
        "model.wv.most_similar(positive=['scientist'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "id": "c9A-m-hyN7iB",
        "outputId": "5cd10549-be27-4a75-9300-455972cbba2a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-90527049-dab6-4be3-9d66-26def32a0914\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-90527049-dab6-4be3-9d66-26def32a0914\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-9dd4a4b3f8d5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#구글 드라이브가 아니라 PC에서 파일을 불러오려면 아래 주석 해제\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;31m# 데이터 불러오기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfile_uploaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#GloVe\n",
        "\n",
        "#구글 드라이브가 아니라 PC에서 파일을 불러오려면 아래 주석 해제\n",
        "from google.colab import files # 데이터 불러오기\n",
        "file_uploaded=files.upload()\n",
        "\n",
        "import numpy as np\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "#glove_file = datapath('/content/drive/MyDrive/Colab Notebooks/glove.6B.100d.txt')  #구글 드라이브에서 파일 경로 복사 후 붙여넣으세요.\n",
        "glove_file = datapath('glove.6B.100d.txt')                                        # PC에서 불러온다면 주석 해제 후 실습\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFrGXy6DQ4CY"
      },
      "outputs": [],
      "source": [
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "model.most_similar('bill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSK9lM2lSA6r"
      },
      "outputs": [],
      "source": [
        "model.most_similar('cherry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UW0QSnnSDj_"
      },
      "outputs": [],
      "source": [
        "model.most_similar(negative='cherry')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fzRWpZbSFaG"
      },
      "outputs": [],
      "source": [
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwwx4q5xSHei"
      },
      "outputs": [],
      "source": [
        "def analogy(x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result[0][0]\n",
        "analogy('australia', 'beer', 'france')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmEni-lySJQy"
      },
      "outputs": [],
      "source": [
        "analogy('tall', 'tallest', 'long')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mJB0-T6SJ_i"
      },
      "outputs": [],
      "source": [
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wC2c_bjUSic_"
      },
      "outputs": [],
      "source": [
        "#아래 BOW 기반의 카운트 벡터 생성은 시간이 부족하면 실습으로 돌림.\n",
        "#시간상 09_01_Project03_ML기반 K-NLP 에서 영어, 한글 같이 처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS2VdF0hAZzo"
      },
      "source": [
        "## BOW 기반의 카운트 벡터 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN_zsr84AZzo",
        "outputId": "0414f259-5f1b-49f4-d334-754165c7aa7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to\n",
            "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\sk8er\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9zdtDTgAZzp",
        "outputId": "fe0dc3ce-20ed-4aa3-cfc3-a41a993373e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#review count: 2000\n",
            "#samples of file ids: ['neg/cv000_29416.txt', 'neg/cv001_19502.txt', 'neg/cv002_17424.txt', 'neg/cv003_12683.txt', 'neg/cv004_12641.txt', 'neg/cv005_29357.txt', 'neg/cv006_17022.txt', 'neg/cv007_4992.txt', 'neg/cv008_29326.txt', 'neg/cv009_29417.txt']\n",
            "#categories of reviews: ['neg', 'pos']\n",
            "#num of \"neg\" reviews: 1000\n",
            "#num of \"pos\" reviews: 1000\n",
            "#id of the first review: neg/cv000_29416.txt\n",
            "#first review content:\n",
            " plot : two teen couples go to a church party , drink and then drive . \n",
            "they get into an accident . \n",
            "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
            "w\n",
            "\n",
            "#sentence tokenization result: [['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.'], ['they', 'get', 'into', 'an', 'accident', '.']]\n",
            "#word tokenization result: ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "print('#review count:', len(movie_reviews.fileids())) #영화 리뷰 문서의 id를 반환\n",
        "print('#samples of file ids:', movie_reviews.fileids()[:10]) #id를 10개까지만 출력\n",
        "print('#categories of reviews:', movie_reviews.categories()) # label, 즉 긍정인지 부정인지에 대한 분류\n",
        "print('#num of \"neg\" reviews:', len(movie_reviews.fileids(categories='neg'))) #label이 부정인 문서들의 id를 반환\n",
        "print('#num of \"pos\" reviews:', len(movie_reviews.fileids(categories='pos'))) #label이 긍정인 문서들의 id를 반환\n",
        "fileid = movie_reviews.fileids()[0] #첫번째 문서의 id를 반환\n",
        "print('#id of the first review:', fileid)\n",
        "print('#first review content:\\n', movie_reviews.raw(fileid)[:200]) #첫번째 문서의 내용을 200자까지만 출력\n",
        "print()\n",
        "print('#sentence tokenization result:', movie_reviews.sents(fileid)[:2]) #첫번째 문서를 sentence tokenize한 결과 중 앞 두 문장\n",
        "print('#word tokenization result:', movie_reviews.words(fileid)[:20]) #첫번째 문서를 word tokenize한 결과 중 앞 스무 단어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCdqn8OmAZzp",
        "outputId": "e15ef764-c9df-422f-a155-88e5bf0608b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch']\n"
          ]
        }
      ],
      "source": [
        "documents = [list(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()]\n",
        "print(documents[0][:50]) #첫째 문서의 앞 50개 단어를 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "engDKSv9AZzp",
        "outputId": "ab3512c4-3d63-4b9d-b5f1-5dd306e8bfc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count of ',': 77717, count of 'the': 76529, count of '.': 65876, count of 'a': 38106, count of 'and': 35576, count of 'of': 34123, count of 'to': 31937, count of ''': 30585, count of 'is': 25195, count of 'in': 21822, "
          ]
        }
      ],
      "source": [
        "word_count = {}\n",
        "for text in documents:\n",
        "    for word in text:\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
        "for word in sorted_features[:10]:\n",
        "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiiwuNRLAZzq",
        "outputId": "42ed92e8-8357-440d-dadb-f90809acb693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num of features: 43030\n",
            "count of 'film': 8935, count of 'one': 5791, count of 'movie': 5538, count of 'like': 3690, count of 'even': 2564, count of 'time': 2409, count of 'good': 2407, count of 'story': 2136, count of 'would': 2084, count of 'much': 2049, "
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords #일반적으로 분석대상이 아닌 단어들\n",
        "\n",
        "tokenizer = RegexpTokenizer(\"[\\w']{3,}\") # 정규포현식으로 토크나이저를 정의\n",
        "english_stops = set(stopwords.words('english')) #영어 불용어를 가져옴\n",
        "\n",
        "#words() 대신 raw()를 이용해 원문을 가져옴\n",
        "documents = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]\n",
        "\n",
        "# stopwords의 적용과 토큰화를 동시에 수행.\n",
        "tokens = [[token for token in tokenizer.tokenize(doc) if token not in english_stops] for doc in documents]\n",
        "\n",
        "word_count = {}\n",
        "for text in tokens:\n",
        "    for word in text:\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "sorted_features = sorted(word_count, key=word_count.get, reverse=True)\n",
        "\n",
        "print('num of features:', len(sorted_features))\n",
        "for word in sorted_features[:10]:\n",
        "    print(f\"count of '{word}': {word_count[word]}\", end=', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kuil-bPYAZzq"
      },
      "outputs": [],
      "source": [
        "word_features = sorted_features[:1000] #빈도가 높은 상위 1000개의 단어만 추출하여 features를 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yCNIuskAZzq",
        "outputId": "3bf4da84-7086-417c-9af9-80fb2b0e6fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "def document_features(document, word_features):\n",
        "    word_count = {}\n",
        "    for word in document: #document에 있는 단어들에 대해 빈도수를 먼저 계산\n",
        "        word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "    features = []\n",
        "    for word in word_features: #word_features의 단어에 대해 계산된 빈도수를 feature에 추가\n",
        "        features.append(word_count.get(word, 0)) #빈도가 없는 단어는 0을 입력\n",
        "    return features\n",
        "\n",
        "word_features_ex = ['one', 'two', 'teen', 'couples', 'solo']\n",
        "doc_ex = ['two', 'two', 'couples']\n",
        "print(document_features(doc_ex, word_features_ex))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YYdUqRAZzq",
        "outputId": "e7b1c112-78c5-4111-e59c-f268f7d90d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(film, 5), (one, 3), (movie, 6), (like, 3), (even, 3), (time, 0), (good, 2), (story, 0), (would, 1), (much, 0), (also, 1), (get, 3), (character, 1), (two, 2), (well, 1), (first, 0), (characters, 1), (see, 2), (way, 3), (make, 5), "
          ]
        }
      ],
      "source": [
        "feature_sets = [document_features(d, word_features) for d in tokens]\n",
        "\n",
        "# 첫째 feature set의 내용을 앞 20개만 word_features의 단어와 함께 출력\n",
        "for i in range(20):\n",
        "    print(f'({word_features[i]}, {feature_sets[0][i]})', end=', ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDN3BGfkAZzq",
        "outputId": "4be085d8-5ff6-451d-e7f0-bba34bdb6253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "print(feature_sets[0][-20:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5vhyzXkAZzr"
      },
      "source": [
        "## 4.3 사이킷런을 이용한 카운트 벡터 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un_aHUH4AZzr"
      },
      "source": [
        "### CountVectorizer\n",
        "\n",
        "http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUDzHmrqAZzr"
      },
      "outputs": [],
      "source": [
        "# data 준비, movie_reviews.raw()를 사용하여 raw text를 추출\n",
        "reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLUpPFbeAZzr",
        "outputId": "b1a8af2d-950c-412a-83e9-94e379c25d67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CountVectorizer(vocabulary=['film', 'one', 'movie', 'like', 'even', 'time',\n",
            "                            'good', 'story', 'would', 'much', 'also', 'get',\n",
            "                            'character', 'two', 'well', 'first', 'characters',\n",
            "                            'see', 'way', 'make', 'life', 'really', 'films',\n",
            "                            'plot', 'little', 'people', 'could', 'bad', 'scene',\n",
            "                            'never', ...])\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#cv = CountVectorizer() #모든 매개변수에 디폴트 값을 사용하는 경우\n",
        "\n",
        "#앞에서 생성한 word_features를 이용하여 특성 집합을 지정하는 경우\n",
        "cv = CountVectorizer(vocabulary=word_features)\n",
        "\n",
        "#cv = CountVectorizer(max_features=1000) #특성 집합을 지정하지 않고 최대 특성의 수를 지정하는 경우\n",
        "print(cv) #객체에 사용된 인수들을 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMxw2XeaAZzr",
        "outputId": "f7a0cf31-369d-4c3d-d475-1e7b257ffb8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['film' 'one' 'movie' 'like' 'even' 'time' 'good' 'story' 'would' 'much'\n",
            " 'also' 'get' 'character' 'two' 'well' 'first' 'characters' 'see' 'way'\n",
            " 'make']\n",
            "['film', 'one', 'movie', 'like', 'even', 'time', 'good', 'story', 'would', 'much', 'also', 'get', 'character', 'two', 'well', 'first', 'characters', 'see', 'way', 'make']\n"
          ]
        }
      ],
      "source": [
        "reviews_cv = cv.fit_transform(reviews) #reviews를 이용하여 count vector를 학습하고, 변환\n",
        "print(cv.get_feature_names_out()[:20]) # count vector에 사용된 feature 이름을 반환\n",
        "print(word_features[:20]) # 비교를 위해 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo5e8wZuAZzr",
        "outputId": "4233daae-44d1-4bd2-a192-84697a89182f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#type of count vectors: <class 'scipy.sparse.csr.csr_matrix'>\n",
            "#shape of count vectors: (2000, 1000)\n",
            "#sample of count vector:\n",
            "  (0, 0)\t6\n",
            "  (0, 1)\t3\n",
            "  (0, 2)\t6\n",
            "  (0, 3)\t3\n",
            "  (0, 4)\t3\n",
            "  (0, 6)\t2\n",
            "  (0, 8)\t1\n"
          ]
        }
      ],
      "source": [
        "print('#type of count vectors:', type(reviews_cv))\n",
        "print('#shape of count vectors:', reviews_cv.shape)\n",
        "print('#sample of count vector:')\n",
        "print(reviews_cv[0, :10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_Ki7AB3AZzr",
        "outputId": "dc69ef80-d562-4750-e80b-544b49c4f456"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<2000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 252984 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz624dnUAZzr",
        "outputId": "1ba254f7-e3cf-47a6-f539-894078205df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5, 3, 6, 3, 3, 0, 2, 0, 1, 0, 1, 3, 1, 2, 1, 0, 1, 2, 3, 5]\n",
            "[6 3 6 3 3 0 2 0 1 0 1 3 2 2 1 0 1 2 3 5]\n"
          ]
        }
      ],
      "source": [
        "print(feature_sets[0][:20]) #절 앞에서 직접 계산한 카운트 벡터\n",
        "print(reviews_cv.toarray()[0, :20]) #변환된 결과의 첫째 feature set 중에서 앞 20개를 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x-DFNGkAZzs",
        "outputId": "5f41375c-629f-40c7-af68-efb863b41b39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "film:6, one:3, movie:6, like:3, even:3, time:0, good:2, story:0, would:1, much:0, also:1, get:3, character:2, two:2, well:1, first:0, characters:1, see:2, way:3, make:5, "
          ]
        }
      ],
      "source": [
        "for word, count in zip(cv.get_feature_names_out()[:20], reviews_cv[0].toarray()[0, :20]):\n",
        "    print(f'{word}:{count}', end=', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN2U_29sAZzs"
      },
      "source": [
        "## 4.4 한국어 텍스트의 카운트 벡터 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3zlnnhGAZzs",
        "outputId": "bdc12312-e80d-4362-a24c-696071e64ec5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>돈 들인건 티가 나지만 보는 내내 하품만</td>\n",
              "      <td>1</td>\n",
              "      <td>2018.10.29</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.</td>\n",
              "      <td>10</td>\n",
              "      <td>2018.10.26</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...</td>\n",
              "      <td>8</td>\n",
              "      <td>2018.10.24</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>이 정도면 볼만하다고 할 수 있음!</td>\n",
              "      <td>8</td>\n",
              "      <td>2018.10.22</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>재미있다</td>\n",
              "      <td>10</td>\n",
              "      <td>2018.10.20</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>나는 재밌게 봄</td>\n",
              "      <td>10</td>\n",
              "      <td>2018.10.14</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.5점은 줄 수 없냐?</td>\n",
              "      <td>0</td>\n",
              "      <td>2018.10.10</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...</td>\n",
              "      <td>10</td>\n",
              "      <td>2018.10.08</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>충격 결말</td>\n",
              "      <td>9</td>\n",
              "      <td>2018.10.06</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>응집력</td>\n",
              "      <td>8</td>\n",
              "      <td>2018.10.05</td>\n",
              "      <td>인피니티 워</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  rating        date  \\\n",
              "0                             돈 들인건 티가 나지만 보는 내내 하품만       1  2018.10.29   \n",
              "1       몰입할수밖에 없다. 어렵게 생각할 필요없다. 내가 전투에 참여한듯 손에 땀이남.      10  2018.10.26   \n",
              "2  이전 작품에 비해 더 화려하고 스케일도 커졌지만.... 전국 맛집의 음식들을 한데 ...       8  2018.10.24   \n",
              "3                                이 정도면 볼만하다고 할 수 있음!       8  2018.10.22   \n",
              "4                                               재미있다      10  2018.10.20   \n",
              "5                                           나는 재밌게 봄      10  2018.10.14   \n",
              "6                                      0.5점은 줄 수 없냐?       0  2018.10.10   \n",
              "7                     헐..다 죽었어....나중에 앤트맨 보다가도 깜놀...      10  2018.10.08   \n",
              "8                                              충격 결말       9  2018.10.06   \n",
              "9                                                응집력       8  2018.10.05   \n",
              "\n",
              "    title  \n",
              "0  인피니티 워  \n",
              "1  인피니티 워  \n",
              "2  인피니티 워  \n",
              "3  인피니티 워  \n",
              "4  인피니티 워  \n",
              "5  인피니티 워  \n",
              "6  인피니티 워  \n",
              "7  인피니티 워  \n",
              "8  인피니티 워  \n",
              "9  인피니티 워  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('./data/daum_movie_review.csv')\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4ZcPo29AZzs",
        "outputId": "77f1c79c-44cf-4bf8-9647-7bbf1ee85435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['10점' '18' '1987' '1도' '1점' '1점도' '2시간' '2시간이' '2편' '5점' '6점' '7점' '8점'\n",
            " 'cg' 'cg가' 'cg는' 'cg도' 'cg만' 'good' 'of' 'ㅋㅋ' 'ㅋㅋㅋ' 'ㅋㅋㅋㅋ' 'ㅎㅎ' 'ㅎㅎㅎ'\n",
            " 'ㅜㅜ' 'ㅠㅠ' 'ㅠㅠㅠ' 'ㅡㅡ' '가는' '가는줄' '가면' '가서' '가슴' '가슴아픈' '가슴이' '가장' '가족'\n",
            " '가족과' '가족들과' '가족의' '가족이' '가지고' '간만에' '갈수록' '감독' '감독님' '감독은' '감독의' '감독이'\n",
            " '감동' '감동과' '감동도' '감동은' '감동을' '감동이' '감동입니다' '감동적' '감동적이고' '감동적인' '감사드립니다'\n",
            " '감사합니다' '감정이' '갑자기' '갔는데' '갔다가' '강철비' '강추' '강추합니다' '같고' '같네요' '같다' '같습니다'\n",
            " '같아' '같아요' '같은' '같은데' '같음' '같이' '개연성' '개연성이' '개인적으로' '거의' '겁나' '것도' '것은'\n",
            " '것을' '것이' '것이다' '겨울왕국' '결국' '결말' '결말이' '계속' '고맙습니다' '곤지암' '공포' '공포를'\n",
            " '공포영화' '관객']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "daum_cv = CountVectorizer(max_features=1000)\n",
        "\n",
        "daum_DTM = daum_cv.fit_transform(df.review) #review를 이용하여 count vector를 학습하고, 변환\n",
        "print(daum_cv.get_feature_names_out()[:100]) # count vector에 사용된 feature 이름을 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jYylAirAZzs",
        "outputId": "cdb2065f-c95f-4fb4-d442-d7026449d85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#전체 형태소 결과: ['몰입', '할수밖에', '없다', '.', '어렵게', '생각', '할', '필요없다', '.', '내', '가', '전투', '에', '참여', '한', '듯', '손', '에', '땀', '이남', '.']\n",
            "#명사만 추출: ['몰입', '생각', '내', '전투', '참여', '듯', '손', '땀', '이남']\n",
            "#품사 태깅 결과 [('몰입', 'Noun'), ('할수밖에', 'Verb'), ('없다', 'Adjective'), ('.', 'Punctuation'), ('어렵게', 'Adjective'), ('생각', 'Noun'), ('할', 'Verb'), ('필요없다', 'Adjective'), ('.', 'Punctuation'), ('내', 'Noun'), ('가', 'Josa'), ('전투', 'Noun'), ('에', 'Josa'), ('참여', 'Noun'), ('한', 'Determiner'), ('듯', 'Noun'), ('손', 'Noun'), ('에', 'Josa'), ('땀', 'Noun'), ('이남', 'Noun'), ('.', 'Punctuation')]\n"
          ]
        }
      ],
      "source": [
        "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
        "twitter_tag = Okt()\n",
        "\n",
        "print('#전체 형태소 결과:', twitter_tag.morphs(df.review[1]))\n",
        "print('#명사만 추출:', twitter_tag.nouns(df.review[1]))\n",
        "print('#품사 태깅 결과', twitter_tag.pos(df.review[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVCUpk8PAZzs",
        "outputId": "4e8d3c5d-be06-4b32-bbe3-dfb3afe05310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "나만의 토크나이저 결과: ['몰입', '할수밖에', '없다', '어렵게', '생각', '할', '필요없다', '내', '전투', '참여', '듯', '손', '땀', '이남']\n"
          ]
        }
      ],
      "source": [
        "def my_tokenizer(doc):\n",
        "    return [token for token, pos in twitter_tag.pos(doc) if pos in ['Noun', 'Verb', 'Adjective']]\n",
        "\n",
        "print(\"나만의 토크나이저 결과:\", my_tokenizer(df.review[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs5J_1k4AZzs",
        "outputId": "c3fb68cf-ecfb-4dea-a809-f6ff4fd590f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['가' '가는' '가는줄' '가면' '가서' '가슴' '가장' '가족' '가족영화' '가지' '가치' '각색' '간' '간다'\n",
            " '간만' '갈' '갈수록' '감' '감독' '감동' '감사' '감사합니다' '감상' '감성' '감정' '감탄' '갑자기' '갔는데'\n",
            " '갔다' '갔다가' '강' '강철' '강추' '같고' '같네요' '같다' '같습니다' '같아' '같아요' '같은' '같은데'\n",
            " '같음' '개' '개그' '개봉' '개연' '개인' '거' '거기' '거리' '거의' '걱정' '건' '건가' '건지' '걸'\n",
            " '겁니다' '것' '게' '겨울왕국' '결론' '결말' '경찰' '경험' '계속' '고' '고맙습니다' '고민' '고생' '곤지암'\n",
            " '곳' '공감' '공포' '공포영화' '과' '과거' '관' '관객' '관객수' '관람' '광주' '괜찮은' '교훈' '구성'\n",
            " '국내' '국민' '군인' '군함도' '굿' '권선' '귀신' '그' '그것' '그게' '그날' '그냥' '그닥' '그대로'\n",
            " '그때' '그래픽']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#토크나이저와 특성의 최대개수를 지정\n",
        "daum_cv = CountVectorizer(max_features=1000, tokenizer=my_tokenizer)\n",
        "#명사만 추출하고 싶은 경우에는 tokenizer에 'twitter_tag.nouns'를 바로 지정해도 됨\n",
        "\n",
        "daum_DTM = daum_cv.fit_transform(df.review) #review를 이용하여 count vector를 학습하고, 변환\n",
        "print(daum_cv.get_feature_names_out()[:100]) # count vector에 사용된 feature 이름을 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLxSNtUTAZzs",
        "outputId": "f9d706ee-9397-4883-ec84-a342fa8a526e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<14725x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 110800 stored elements in Compressed Sparse Row format>\n",
            "0.007524617996604414\n"
          ]
        }
      ],
      "source": [
        "print(repr(daum_DTM))\n",
        "print(110800/(14725*1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1E6GreMAZzs",
        "outputId": "7a7245b4-16d8-4f46-a459-a5b3d0dc8d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "내 : 1, 듯 : 1, 몰입 : 1, 생각 : 1, 손 : 1, 없다 : 1, 할 : 1, "
          ]
        }
      ],
      "source": [
        "for word, count in zip(daum_cv.get_feature_names_out(), daum_DTM[1].toarray()[0]):\n",
        "    if count > 0:\n",
        "        print(word, ':', count, end=', ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBeoZdsSAZzs"
      },
      "source": [
        "## 4.5 카운트 벡터의 활용\n",
        "\n",
        "### 코사인 유사도(Cosine similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSKXUiWAAZzs",
        "outputId": "179848ce-65fa-4c10-e49b-b64bbf2b71ce"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# 그래프에서 마이너스 폰트 깨지는 문제에 대한 대처\n",
        "mpl.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "import numpy as np\n",
        "x = np.arange(0,2*np.pi,0.1)   # start,stop,step\n",
        "y = np.cos(x)\n",
        "#print(x)\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXhSabrOAZzs",
        "outputId": "9a97bc6a-4a02-4c14-f26c-0981b98e7c97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#대상 특성 행렬의 크기: (1, 1000)\n",
            "#유사도 계산 행렬의 크기: (1, 2000)\n",
            "#유사도 계산결과를 역순으로 정렬: [0.8367205630128807, 0.43817531290756406, 0.4080451370075411, 0.40727044884302327, 0.4060219836225451, 0.3999621981759778, 0.39965783997760135, 0.39566661804603703, 0.3945302295079114, 0.3911637170821695]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "start = len(reviews[0]) // 2 #첫째 리뷰의 문자수를 확인하고 뒤 절반을 가져오기 위해 중심점을 찾음\n",
        "source = reviews[0][-start:] #중심점으로부터 뒤 절반을 가져와서 비교할 문서를 생성\n",
        "\n",
        "source_cv = cv.transform([source]) #코사인 유사도는 카운트 벡터에 대해 계산하므로 벡터로 변환\n",
        "#transform은 반드시 리스트나 행렬 형태의 입력을 요구하므로 리스트로 만들어서 입력\n",
        "\n",
        "print(\"#대상 특성 행렬의 크기:\", source_cv.shape) #행렬의 크기를 확인, 문서가 하나이므로 (1, 1000)\n",
        "\n",
        "sim_result = cosine_similarity(source_cv, reviews_cv) #변환된 count vector와 기존 값들과의 similarity 계산\n",
        "\n",
        "print(\"#유사도 계산 행렬의 크기:\", sim_result.shape)\n",
        "print(\"#유사도 계산결과를 역순으로 정렬:\", sorted(sim_result[0], reverse=True)[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-96RS0mkAZzs",
        "outputId": "de4d223f-0d9f-48bd-9546-7cd66c0b48b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#가장 유사한 리뷰의 인덱스: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print('#가장 유사한 리뷰의 인덱스:', np.argmax(sim_result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0riXGmbiAZzt",
        "outputId": "b88ab605-616e-454c-ab2c-590ec500fb7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#가장 유사한 리뷰부터 정렬한 인덱스: [   0 1110 1570  687  628  112 1712 1393  524 1740]\n"
          ]
        }
      ],
      "source": [
        "print('#가장 유사한 리뷰부터 정렬한 인덱스:', (-sim_result[0]).argsort()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWayXW2OAZzt"
      },
      "source": [
        "## 4.6 TF-IDF로 성능을 높여보자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx-HCLBbAZzt",
        "outputId": "d90a7fc6-6e91-4004-caeb-8e3bc776e070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#shape of tfidf matrix: (2000, 1000)\n",
            "#20 count score of the first review: [6 3 6 3 3 0 2 0 1 0 1 3 2 2 1 0 1 2 3 5]\n",
            "#20 tfidf score of the first review: [0.13556199 0.06700076 0.14998642 0.0772298  0.08608998 0.\n",
            " 0.0609124  0.         0.03126552 0.         0.03242315 0.09567082\n",
            " 0.06575035 0.06518293 0.03225625 0.         0.0345017  0.06863314\n",
            " 0.10042383 0.16727495]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "transformer = TfidfTransformer()\n",
        "transformer\n",
        "\n",
        "reviews_tfidf = transformer.fit_transform(reviews_cv)\n",
        "print('#shape of tfidf matrix:', reviews_tfidf.shape) #TF-IDF 행렬의 모양과 카운트 행렬의 모양이 일치하는 것을 확인\n",
        "\n",
        "#첫 리뷰의 카운트 벡터 중 앞 20개 값 출력\n",
        "print('#20 count score of the first review:', reviews_cv[0].toarray()[0][:20])\n",
        "#첫 리뷰의 TF-IDF 벡터 중 앞 20개 값 출력\n",
        "print('#20 tfidf score of the first review:', reviews_tfidf[0].toarray()[0][:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaS76s4cAZzt",
        "outputId": "7890e9da-bfc5-4bf8-95d5-e994b0030c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#가장 유사한 리뷰의 인덱스: 0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tf = TfidfVectorizer(vocabulary=word_features)\n",
        "reviews_tf = tf.fit_transform(reviews)\n",
        "\n",
        "source_tf = tf.transform([source]) #코사인 유사도는 카운트 벡터에 대해 계산하므로 벡터로 변환\n",
        "#transform은 반드시 리스트나 행렬 형태의 입력을 요구하므로 리스트로 만들어서 입력\n",
        "\n",
        "sim_result_tf = cosine_similarity(source_tf, reviews_tf) #변환된 count vector와 기존 값들과의 similarity 계산\n",
        "\n",
        "print('#가장 유사한 리뷰의 인덱스:', np.argmax(sim_result_tf[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MgtWhanAZzt",
        "outputId": "8038ad42-b969-43b5-925c-e5dffc1e1ad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#카운트 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스: [   0 1110 1570  687  628  112 1712 1393  524 1740]\n",
            "#TF-IDF 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스: [   0 1110 1393 1570  645  323 1143  628 1676 1391]\n"
          ]
        }
      ],
      "source": [
        "print('#카운트 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스:', (-sim_result[0]).argsort()[:10])\n",
        "print('#TF-IDF 벡터에 대해 가장 유사한 리뷰부터 정렬한 인덱스:', (-sim_result_tf[0]).argsort()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5h3v88hW1I7"
      },
      "source": [
        "# 추가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8M8XPAk3AZzt"
      },
      "outputs": [],
      "source": [
        "#TF\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "\n",
        "corpus = ['Time flies like an arrow.',\n",
        "          'Fruit flies like a banana.']\n",
        "one_hot_vectorizer = CountVectorizer(binary=True)\n",
        "one_hot = one_hot_vectorizer.fit_transform(corpus).toarray()\n",
        "vocab = one_hot_vectorizer.get_feature_names()\n",
        "sns.heatmap(one_hot, annot=True,\n",
        "            cbar=False, xticklabels=vocab,\n",
        "            yticklabels=['Sentence 1', 'Sentence 2'])\n",
        "\n",
        "# plt.savefig('1-04.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rq_4_PlZXHx1"
      },
      "outputs": [],
      "source": [
        "#TF-IDF 표현\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
        "sns.heatmap(tfidf, annot=True, cbar=False, xticklabels=vocab,\n",
        "            yticklabels= ['Sentence 1', 'Sentence 2'])\n",
        "\n",
        "# plt.savefig('1-05.png', dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnQpDl29XR2W"
      },
      "source": [
        "### 설명\n",
        "첫 번째 문장의 'flies'와 'like'의 경우 TF = 1이므로 $\\text{TF-IDF}=1\\times\\text{log}\\left(\\dfrac{2+1}{2+1}\\right)+1=1$입니다.\n",
        "\n",
        "단어 'an', 'arrow', 'time'의 경우 $N_w=1$입니다. 따라서 $\\text{TF-IDF}=1\\times\\text{log}\\left(\\dfrac{2+1}{1+1}\\right)+1=1.4054651081081644$입니다.\n",
        "\n",
        "L2 정규화를 적용하면 'flies'와 'like'는 $\\dfrac{1}{\\sqrt{2\\times1^2+3\\times1.4054651081081644^2+}}=0.3552$가 됩니다.\n",
        "\n",
        "'an', 'arrow', 'time'는 $\\dfrac{1.4054651081081644}{\\sqrt{2\\times1^2+3\\times1.4054651081081644^2+}}=0.4992$가 됩니다."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}