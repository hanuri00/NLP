{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM1drlqEMpcEdyhcbtgiq1o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"IIFGVx_EqypG"},"outputs":[],"source":[]},{"cell_type":"code","source":["!pip install -q torch torchvision transformers kogpt2 pytorch_lightning"],"metadata":{"id":"Kb77EN6rq0Ph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n","from kogpt2_transformers import get_kogpt2_tokenizer"],"metadata":{"id":"04e3sQUsq0SS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KoGPT 모델 학습 데이터셋 정의\n","class MyDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_len):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_tensors=\"pt\"\n","        )\n","        input_ids = inputs[\"input_ids\"].squeeze()\n","        attention_mask = inputs[\"attention_mask\"].squeeze()\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"attention_mask\": attention_mask\n","        }"],"metadata":{"id":"4gaRcKuoq0U5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습할 데이터 준비\n","train_texts = [\"안녕하세요, 반갑습니다.\", \"오늘 날씨가 좋네요.\", \"저는 한국어 문장 생성을 배우고 있어요.\"]\n","val_texts = [\"좋은 하루 되세요.\", \"내일 뭐 할까요?\"]\n","\n","#x 토그나이저, 데이터셋\n","tokenizer = get_kogpt2_tokenizer()\n","train_dataset = MyDataset(train_texts, tokenizer, max_len=32)\n","val_dataset = MyDataset(val_texts, tokenizer, max_len=32)\n","\n","# 모델 학습 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n","model.to(device)\n","\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","criterion = torch.nn.CrossEntropyLoss()"],"metadata":{"id":"j-PK8DnDq0YN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0.0\n","\n","    for batch in train_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n","        loss = outputs.loss\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","    model.eval()\n","    val_loss = 0.0\n","\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n","            loss = outputs.loss\n","\n","            val_loss += loss.item()\n","\n","        val_loss /= len(val_loader)\n","\n","    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.7f} | Val Loss: {val_loss:.7f}\")"],"metadata":{"id":"CajULgzTq0bK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장 생성 예측 함수 정의\n","def generate_sentence(prompt, max_length=50, temperature=0.7):\n","    model.eval()\n","    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n","    output = model.generate(input_ids, max_length=max_length, temperature=temperature)\n","    generated_sentence = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return generated_sentence"],"metadata":{"id":"dJylYU9_q0dw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 예시 문장 생성\n","prompt = \"안녕하세요,\"\n","generated_sentence = generate_sentence(prompt)\n","print(generated_sentence)"],"metadata":{"id":"dwOsyqf6q0gY"},"execution_count":null,"outputs":[]}]}